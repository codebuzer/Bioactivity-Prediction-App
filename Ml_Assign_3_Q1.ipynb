{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNKMkJ5RZ/9FZpHMpQaE4P/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codebuzer/Bioactivity-Prediction-App/blob/main/Ml_Assign_3_Q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4DUfpLxcXC8",
        "outputId": "830eda1f-233b-49f6-bdaa-f0fd8b7a2c83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import wandb\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Performance optimizations for faster training\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "# Configure matplotlib\n",
        "%matplotlib inline\n",
        "plt.close('all')\n",
        "import datetime\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LightCNN(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.3):\n",
        "        super(LightCNN, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            # First block\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            # Second block\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            # Third block\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(128 * 4 * 4, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(512, 100)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "def get_cifar100_classes():\n",
        "    return [\n",
        "        'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',\n",
        "        'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',\n",
        "        'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',\n",
        "        'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
        "        'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',\n",
        "        'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
        "        'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
        "        'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
        "        'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
        "        'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea',\n",
        "        'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
        "        'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
        "        'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
        "        'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman',\n",
        "        'worm'\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "ukOipzcRclvh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_prepare_data(batch_size):\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "        transforms.RandomAffine( degrees=0,  translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                            download=True, transform=train_transform)\n",
        "    testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                           download=True, transform=test_transform)\n",
        "\n",
        "    trainset.classes = get_cifar100_classes()\n",
        "    testset.classes = get_cifar100_classes()\n",
        "\n",
        "    train_size = int(0.9 * len(trainset))\n",
        "    val_size = len(trainset) - train_size\n",
        "    trainset, valset = torch.utils.data.random_split(trainset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True,\n",
        "                            num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False,\n",
        "                           num_workers=2, pin_memory=True)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "def show_random_images(dataset, run, num_classes=10, images_per_class=5):\n",
        "    \"\"\"Modified to use existing wandb run\"\"\"\n",
        "    if hasattr(dataset, 'dataset'):\n",
        "        classes = dataset.dataset.classes if hasattr(dataset.dataset, 'classes') else None\n",
        "    else:\n",
        "        classes = dataset.classes if hasattr(dataset, 'classes') else None\n",
        "\n",
        "    fig = plt.figure(figsize=(15, 30))\n",
        "\n",
        "    selected_classes = np.random.choice(100, num_classes, replace=False)\n",
        "    class_images = {i: [] for i in selected_classes}\n",
        "\n",
        "    for img, label in dataset:\n",
        "        if label in selected_classes and len(class_images[label]) < images_per_class:\n",
        "            class_images[label].append(img)\n",
        "        if all(len(imgs) == images_per_class for imgs in class_images.values()):\n",
        "            break\n",
        "\n",
        "    for idx, class_idx in enumerate(selected_classes):\n",
        "        for j, img in enumerate(class_images[class_idx]):\n",
        "            plt.subplot(num_classes, images_per_class, idx * images_per_class + j + 1)\n",
        "            img = img.numpy().transpose(1, 2, 0)\n",
        "            img = img * np.array([0.5, 0.5, 0.5]) + np.array([0.5, 0.5, 0.5])\n",
        "            img = np.clip(img, 0, 1)\n",
        "            plt.imshow(img)\n",
        "            plt.axis('off')\n",
        "            if j == 0:\n",
        "                if classes is not None:\n",
        "                    plt.title(f'Class: {classes[class_idx]}')\n",
        "                else:\n",
        "                    plt.title(f'Class {class_idx}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/drive/MyDrive/ML Assignment 3/random_images.png')\n",
        "    run.log({\"dataset_samples\": wandb.Image(plt)})\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "pgEB6goRcpqz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_loss = running_loss / len(val_loader)\n",
        "    val_acc = 100. * correct / total\n",
        "\n",
        "    return val_loss, val_acc\n",
        "\n",
        "def train_and_log(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, run):\n",
        "    \"\"\"Modified training function to use existing wandb run\"\"\"\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    best_val_acc = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}] Batch [{i}/{len(train_loader)}]')\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100. * correct / total\n",
        "\n",
        "        val_loss, val_acc = validate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        # Log metrics\n",
        "        run.log({\n",
        "            \"epoch\": epoch,\n",
        "            \"train/loss\": train_loss,\n",
        "            \"train/accuracy\": train_acc,\n",
        "            \"val/loss\": val_loss,\n",
        "            \"val/accuracy\": val_acc\n",
        "        })\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(),\n",
        "                      f'/content/drive/MyDrive/ML Assignment 3/best_model_{run.id}.pth')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def generate_classification_report(model, test_loader, device, config, save_path, run):\n",
        "    \"\"\"Modified to use existing wandb run\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    class_names = get_cifar100_classes()\n",
        "    report = classification_report(all_labels, all_preds,\n",
        "                                 target_names=class_names,\n",
        "                                 digits=4)\n",
        "\n",
        "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    report_filename = f'classification_report_{config[\"optimizer\"]}_lr_{config[\"lr\"]}_{timestamp}.txt'\n",
        "    report_path = os.path.join(save_path, report_filename)\n",
        "\n",
        "    with open(report_path, 'w') as f:\n",
        "        f.write(f\"Configuration:\\n{config}\\n\\n\")\n",
        "        f.write(report)\n",
        "\n",
        "    # Log to wandb\n",
        "    run.log({\n",
        "        \"classification_report\": wandb.Table(\n",
        "            columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"],\n",
        "            data=[[class_names[i], *map(float, row.strip().split()[1:-1]),\n",
        "                  int(row.strip().split()[-1])]\n",
        "                 for i, row in enumerate(report.split('\\n')[2:-5])]\n",
        "        )\n",
        "    })\n",
        "\n",
        "    # Create and log confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    cm_filename = f'confusion_matrix_{config[\"optimizer\"]}_lr_{config[\"lr\"]}_{timestamp}.png'\n",
        "    cm_path = os.path.join(save_path, cm_filename)\n",
        "    plt.savefig(cm_path)\n",
        "    run.log({\"confusion_matrix\": wandb.Image(cm_path)})\n",
        "    plt.close()\n",
        "\n",
        "    return report\n",
        "\n",
        "def main():\n",
        "    # Login to wandb\n",
        "    wandb.login()\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    reports_dir = '/content/drive/MyDrive/ML Assignment 3/cifar100_reports'\n",
        "    os.makedirs(reports_dir, exist_ok=True)\n",
        "\n",
        "    configs = [\n",
        "        {\"lr\": 0.001, \"batch_size\": 128, \"optimizer\": \"adam\", \"dropout\": 0.3, \"epochs\": 20},\n",
        "        {\"lr\": 0.01, \"batch_size\": 32, \"optimizer\": \"sgd\", \"dropout\": 0.3, \"epochs\": 20},\n",
        "        {\"lr\": 0.0001, \"batch_size\": 256, \"optimizer\": \"adam\", \"dropout\": 0.7,\"epochs\":20},\n",
        "        {\"lr\": 0.005, \"batch_size\": 128, \"optimizer\": \"sgd\", \"dropout\": 0.4,\"epochs\":20},\n",
        "        {\"lr\": 0.002, \"batch_size\": 32, \"optimizer\": \"adam\", \"dropout\": 0.6,\"epochs\":20}\n",
        "    ]\n",
        "\n",
        "    for config in configs:\n",
        "        # Initialize wandb run at the start of each configuration\n",
        "        with wandb.init(project=\"cifar100-cnn\", config=config, reinit=True) as run:\n",
        "            print(f\"\\nTraining with config: {config}\")\n",
        "\n",
        "            train_loader, val_loader, test_loader = load_and_prepare_data(config[\"batch_size\"])\n",
        "\n",
        "            # Show random images at the start\n",
        "            print(\"Generating random image samples...\")\n",
        "            show_random_images(train_loader.dataset, run=run)\n",
        "\n",
        "            model = LightCNN(dropout_rate=config[\"dropout\"]).to(device)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            if config[\"optimizer\"] == \"adam\":\n",
        "                optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
        "            else:\n",
        "                optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"],\n",
        "                                    momentum=0.9, nesterov=True)\n",
        "\n",
        "            # Train model\n",
        "            model = train_and_log(model, train_loader, val_loader, criterion,\n",
        "                                optimizer, config[\"epochs\"], device, run)\n",
        "\n",
        "            # Generate classification report\n",
        "            print(\"\\nGenerating classification report...\")\n",
        "            report = generate_classification_report(model, test_loader, device,\n",
        "                                                 config, reports_dir, run)\n",
        "            print(\"\\nClassification Report:\")\n",
        "            print(report)\n",
        "            print(f\"\\nReport saved to {reports_dir}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Lip8oBoud9by",
        "outputId": "efd29f9c-1975-499a-911d-5905e4d07366"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250322_201419-4q9gkawj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/4q9gkawj' target=\"_blank\">helpful-shape-11</a></strong> to <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn' target=\"_blank\">https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/4q9gkawj' target=\"_blank\">https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/4q9gkawj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with config: {'lr': 0.001, 'batch_size': 128, 'optimizer': 'adam', 'dropout': 0.3, 'epochs': 20}\n",
            "Generating random image samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-0b4a4a8ed1dd>:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "<ipython-input-10-0b4a4a8ed1dd>:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] Batch [0/352]\n",
            "Epoch [1/20] Batch [100/352]\n",
            "Epoch [1/20] Batch [200/352]\n",
            "Epoch [1/20] Batch [300/352]\n",
            "Epoch [1/20]\n",
            "Train Loss: 3.9664, Train Acc: 8.92%\n",
            "Val Loss: 3.6096, Val Acc: 14.04%\n",
            "Epoch [2/20] Batch [0/352]\n",
            "Epoch [2/20] Batch [100/352]\n",
            "Epoch [2/20] Batch [200/352]\n",
            "Epoch [2/20] Batch [300/352]\n",
            "Epoch [2/20]\n",
            "Train Loss: 3.4936, Train Acc: 16.12%\n",
            "Val Loss: 3.3227, Val Acc: 19.40%\n",
            "Epoch [3/20] Batch [0/352]\n",
            "Epoch [3/20] Batch [100/352]\n",
            "Epoch [3/20] Batch [200/352]\n",
            "Epoch [3/20] Batch [300/352]\n",
            "Epoch [3/20]\n",
            "Train Loss: 3.2748, Train Acc: 19.91%\n",
            "Val Loss: 3.0896, Val Acc: 23.94%\n",
            "Epoch [4/20] Batch [0/352]\n",
            "Epoch [4/20] Batch [100/352]\n",
            "Epoch [4/20] Batch [200/352]\n",
            "Epoch [4/20] Batch [300/352]\n",
            "Epoch [4/20]\n",
            "Train Loss: 3.1166, Train Acc: 22.74%\n",
            "Val Loss: 2.9759, Val Acc: 25.70%\n",
            "Epoch [5/20] Batch [0/352]\n",
            "Epoch [5/20] Batch [100/352]\n",
            "Epoch [5/20] Batch [200/352]\n",
            "Epoch [5/20] Batch [300/352]\n",
            "Epoch [5/20]\n",
            "Train Loss: 3.0206, Train Acc: 24.47%\n",
            "Val Loss: 2.8176, Val Acc: 29.50%\n",
            "Epoch [6/20] Batch [0/352]\n",
            "Epoch [6/20] Batch [100/352]\n",
            "Epoch [6/20] Batch [200/352]\n",
            "Epoch [6/20] Batch [300/352]\n",
            "Epoch [6/20]\n",
            "Train Loss: 2.9224, Train Acc: 26.54%\n",
            "Val Loss: 2.7752, Val Acc: 30.38%\n",
            "Epoch [7/20] Batch [0/352]\n",
            "Epoch [7/20] Batch [100/352]\n",
            "Epoch [7/20] Batch [200/352]\n",
            "Epoch [7/20] Batch [300/352]\n",
            "Epoch [7/20]\n",
            "Train Loss: 2.8519, Train Acc: 27.80%\n",
            "Val Loss: 2.7166, Val Acc: 31.82%\n",
            "Epoch [8/20] Batch [0/352]\n",
            "Epoch [8/20] Batch [100/352]\n",
            "Epoch [8/20] Batch [200/352]\n",
            "Epoch [8/20] Batch [300/352]\n",
            "Epoch [8/20]\n",
            "Train Loss: 2.7983, Train Acc: 28.74%\n",
            "Val Loss: 2.7104, Val Acc: 31.42%\n",
            "Epoch [9/20] Batch [0/352]\n",
            "Epoch [9/20] Batch [100/352]\n",
            "Epoch [9/20] Batch [200/352]\n",
            "Epoch [9/20] Batch [300/352]\n",
            "Epoch [9/20]\n",
            "Train Loss: 2.7342, Train Acc: 30.22%\n",
            "Val Loss: 2.6109, Val Acc: 32.82%\n",
            "Epoch [10/20] Batch [0/352]\n",
            "Epoch [10/20] Batch [100/352]\n",
            "Epoch [10/20] Batch [200/352]\n",
            "Epoch [10/20] Batch [300/352]\n",
            "Epoch [10/20]\n",
            "Train Loss: 2.6928, Train Acc: 31.11%\n",
            "Val Loss: 2.5643, Val Acc: 33.50%\n",
            "Epoch [11/20] Batch [0/352]\n",
            "Epoch [11/20] Batch [100/352]\n",
            "Epoch [11/20] Batch [200/352]\n",
            "Epoch [11/20] Batch [300/352]\n",
            "Epoch [11/20]\n",
            "Train Loss: 2.6539, Train Acc: 31.83%\n",
            "Val Loss: 2.5285, Val Acc: 35.04%\n",
            "Epoch [12/20] Batch [0/352]\n",
            "Epoch [12/20] Batch [100/352]\n",
            "Epoch [12/20] Batch [200/352]\n",
            "Epoch [12/20] Batch [300/352]\n",
            "Epoch [12/20]\n",
            "Train Loss: 2.6097, Train Acc: 32.65%\n",
            "Val Loss: 2.5075, Val Acc: 36.72%\n",
            "Epoch [13/20] Batch [0/352]\n",
            "Epoch [13/20] Batch [100/352]\n",
            "Epoch [13/20] Batch [200/352]\n",
            "Epoch [13/20] Batch [300/352]\n",
            "Epoch [13/20]\n",
            "Train Loss: 2.5775, Train Acc: 33.30%\n",
            "Val Loss: 2.4595, Val Acc: 37.14%\n",
            "Epoch [14/20] Batch [0/352]\n",
            "Epoch [14/20] Batch [100/352]\n",
            "Epoch [14/20] Batch [200/352]\n",
            "Epoch [14/20] Batch [300/352]\n",
            "Epoch [14/20]\n",
            "Train Loss: 2.5465, Train Acc: 33.91%\n",
            "Val Loss: 2.4430, Val Acc: 37.26%\n",
            "Epoch [15/20] Batch [0/352]\n",
            "Epoch [15/20] Batch [100/352]\n",
            "Epoch [15/20] Batch [200/352]\n",
            "Epoch [15/20] Batch [300/352]\n",
            "Epoch [15/20]\n",
            "Train Loss: 2.5100, Train Acc: 34.56%\n",
            "Val Loss: 2.4177, Val Acc: 38.14%\n",
            "Epoch [16/20] Batch [0/352]\n",
            "Epoch [16/20] Batch [100/352]\n",
            "Epoch [16/20] Batch [200/352]\n",
            "Epoch [16/20] Batch [300/352]\n",
            "Epoch [16/20]\n",
            "Train Loss: 2.5001, Train Acc: 34.74%\n",
            "Val Loss: 2.4057, Val Acc: 38.06%\n",
            "Epoch [17/20] Batch [0/352]\n",
            "Epoch [17/20] Batch [100/352]\n",
            "Epoch [17/20] Batch [200/352]\n",
            "Epoch [17/20] Batch [300/352]\n",
            "Epoch [17/20]\n",
            "Train Loss: 2.4637, Train Acc: 35.66%\n",
            "Val Loss: 2.3414, Val Acc: 39.04%\n",
            "Epoch [18/20] Batch [0/352]\n",
            "Epoch [18/20] Batch [100/352]\n",
            "Epoch [18/20] Batch [200/352]\n",
            "Epoch [18/20] Batch [300/352]\n",
            "Epoch [18/20]\n",
            "Train Loss: 2.4413, Train Acc: 36.02%\n",
            "Val Loss: 2.3341, Val Acc: 39.64%\n",
            "Epoch [19/20] Batch [0/352]\n",
            "Epoch [19/20] Batch [100/352]\n",
            "Epoch [19/20] Batch [200/352]\n",
            "Epoch [19/20] Batch [300/352]\n",
            "Epoch [19/20]\n",
            "Train Loss: 2.4288, Train Acc: 36.36%\n",
            "Val Loss: 2.3604, Val Acc: 39.60%\n",
            "Epoch [20/20] Batch [0/352]\n",
            "Epoch [20/20] Batch [100/352]\n",
            "Epoch [20/20] Batch [200/352]\n",
            "Epoch [20/20] Batch [300/352]\n",
            "Epoch [20/20]\n",
            "Train Loss: 2.3978, Train Acc: 37.04%\n",
            "Val Loss: 2.2720, Val Acc: 40.10%\n",
            "\n",
            "Generating classification report...\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        apple     0.6571    0.6900    0.6732       100\n",
            "aquarium_fish     0.5644    0.5700    0.5672       100\n",
            "         baby     0.4697    0.3100    0.3735       100\n",
            "         bear     0.3043    0.1400    0.1918       100\n",
            "       beaver     0.2759    0.0800    0.1240       100\n",
            "          bed     0.3828    0.4900    0.4298       100\n",
            "          bee     0.5259    0.6100    0.5648       100\n",
            "       beetle     0.6250    0.3000    0.4054       100\n",
            "      bicycle     0.5670    0.5500    0.5584       100\n",
            "       bottle     0.6458    0.6200    0.6327       100\n",
            "         bowl     0.3867    0.2900    0.3314       100\n",
            "          boy     0.2876    0.4400    0.3478       100\n",
            "       bridge     0.5714    0.2800    0.3758       100\n",
            "          bus     0.2824    0.4800    0.3556       100\n",
            "    butterfly     0.3429    0.4800    0.4000       100\n",
            "        camel     0.3486    0.3800    0.3636       100\n",
            "          can     0.4240    0.5300    0.4711       100\n",
            "       castle     0.5714    0.6400    0.6038       100\n",
            "  caterpillar     0.3798    0.4900    0.4279       100\n",
            "       cattle     0.4875    0.3900    0.4333       100\n",
            "        chair     0.7692    0.7000    0.7330       100\n",
            "   chimpanzee     0.4803    0.7300    0.5794       100\n",
            "        clock     0.4353    0.3700    0.4000       100\n",
            "        cloud     0.6778    0.6100    0.6421       100\n",
            "    cockroach     0.6827    0.7100    0.6961       100\n",
            "        couch     0.3956    0.3600    0.3770       100\n",
            "         crab     0.4677    0.2900    0.3580       100\n",
            "    crocodile     0.3187    0.2900    0.3037       100\n",
            "          cup     0.7632    0.5800    0.6591       100\n",
            "     dinosaur     0.4024    0.3300    0.3626       100\n",
            "      dolphin     0.3603    0.4900    0.4153       100\n",
            "     elephant     0.5098    0.5200    0.5149       100\n",
            "     flatfish     0.4200    0.4200    0.4200       100\n",
            "       forest     0.5244    0.4300    0.4725       100\n",
            "          fox     0.4598    0.4000    0.4278       100\n",
            "         girl     0.3056    0.2200    0.2558       100\n",
            "      hamster     0.3893    0.5800    0.4659       100\n",
            "        house     0.3007    0.4600    0.3636       100\n",
            "     kangaroo     0.3906    0.2500    0.3049       100\n",
            "     keyboard     0.7303    0.6500    0.6878       100\n",
            "         lamp     0.6875    0.3300    0.4459       100\n",
            "   lawn_mower     0.5763    0.6800    0.6239       100\n",
            "      leopard     0.7750    0.3100    0.4429       100\n",
            "         lion     0.7200    0.3600    0.4800       100\n",
            "       lizard     0.2674    0.2300    0.2473       100\n",
            "      lobster     0.3519    0.1900    0.2468       100\n",
            "          man     0.4571    0.1600    0.2370       100\n",
            "   maple_tree     0.6053    0.4600    0.5227       100\n",
            "   motorcycle     0.4590    0.8400    0.5936       100\n",
            "     mountain     0.7209    0.6200    0.6667       100\n",
            "        mouse     0.2348    0.2700    0.2512       100\n",
            "     mushroom     0.4021    0.3900    0.3959       100\n",
            "     oak_tree     0.5133    0.7700    0.6160       100\n",
            "       orange     0.6942    0.8400    0.7602       100\n",
            "       orchid     0.5285    0.6500    0.5830       100\n",
            "        otter     0.1034    0.0300    0.0465       100\n",
            "    palm_tree     0.6765    0.6900    0.6832       100\n",
            "         pear     0.5714    0.6000    0.5854       100\n",
            " pickup_truck     0.3511    0.6600    0.4583       100\n",
            "    pine_tree     0.6200    0.3100    0.4133       100\n",
            "        plain     0.7629    0.7400    0.7513       100\n",
            "        plate     0.4919    0.6100    0.5446       100\n",
            "        poppy     0.5500    0.5500    0.5500       100\n",
            "    porcupine     0.5902    0.3600    0.4472       100\n",
            "       possum     0.1871    0.3200    0.2362       100\n",
            "       rabbit     0.3830    0.1800    0.2449       100\n",
            "      raccoon     0.2090    0.5600    0.3043       100\n",
            "          ray     0.3786    0.3900    0.3842       100\n",
            "         road     0.7719    0.8800    0.8224       100\n",
            "       rocket     0.6354    0.6100    0.6224       100\n",
            "         rose     0.5057    0.4400    0.4706       100\n",
            "          sea     0.5965    0.6800    0.6355       100\n",
            "         seal     0.2174    0.1000    0.1370       100\n",
            "        shark     0.3265    0.3200    0.3232       100\n",
            "        shrew     0.2747    0.2500    0.2618       100\n",
            "        skunk     0.3857    0.8100    0.5226       100\n",
            "   skyscraper     0.7157    0.7300    0.7228       100\n",
            "        snail     0.3214    0.1800    0.2308       100\n",
            "        snake     0.2418    0.3700    0.2925       100\n",
            "       spider     0.4348    0.3000    0.3550       100\n",
            "     squirrel     0.2692    0.2100    0.2360       100\n",
            "    streetcar     0.3768    0.5200    0.4370       100\n",
            "    sunflower     0.8539    0.7600    0.8042       100\n",
            " sweet_pepper     0.3659    0.3000    0.3297       100\n",
            "        table     0.4800    0.4800    0.4800       100\n",
            "         tank     0.7059    0.4800    0.5714       100\n",
            "    telephone     0.4898    0.4800    0.4848       100\n",
            "   television     0.4656    0.6100    0.5281       100\n",
            "        tiger     0.5843    0.5200    0.5503       100\n",
            "      tractor     0.4000    0.6400    0.4923       100\n",
            "        train     0.4598    0.4000    0.4278       100\n",
            "        trout     0.4218    0.6200    0.5020       100\n",
            "        tulip     0.3040    0.3800    0.3378       100\n",
            "       turtle     0.4565    0.2100    0.2877       100\n",
            "     wardrobe     0.8276    0.7200    0.7701       100\n",
            "        whale     0.5402    0.4700    0.5027       100\n",
            "  willow_tree     0.4912    0.2800    0.3567       100\n",
            "         wolf     0.2439    0.4000    0.3030       100\n",
            "        woman     0.2500    0.1900    0.2159       100\n",
            "         worm     0.4896    0.4700    0.4796       100\n",
            "\n",
            "     accuracy                         0.4606     10000\n",
            "    macro avg     0.4766    0.4606    0.4533     10000\n",
            " weighted avg     0.4766    0.4606    0.4533     10000\n",
            "\n",
            "\n",
            "Report saved to /content/drive/MyDrive/ML Assignment 3/cifar100_reports\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train/accuracy</td><td>▁▃▄▄▅▅▆▆▆▇▇▇▇▇▇▇████</td></tr><tr><td>train/loss</td><td>█▆▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val/accuracy</td><td>▁▂▄▄▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>val/loss</td><td>█▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train/accuracy</td><td>37.04222</td></tr><tr><td>train/loss</td><td>2.39777</td></tr><tr><td>val/accuracy</td><td>40.1</td></tr><tr><td>val/loss</td><td>2.27199</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">helpful-shape-11</strong> at: <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/4q9gkawj' target=\"_blank\">https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/4q9gkawj</a><br> View project at: <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn' target=\"_blank\">https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn</a><br>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250322_201419-4q9gkawj/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250322_203406-4k4ayn5c</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/4k4ayn5c' target=\"_blank\">misty-butterfly-12</a></strong> to <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn' target=\"_blank\">https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/4k4ayn5c' target=\"_blank\">https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/4k4ayn5c</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with config: {'lr': 0.01, 'batch_size': 32, 'optimizer': 'sgd', 'dropout': 0.3, 'epochs': 20}\n",
            "Generating random image samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-0b4a4a8ed1dd>:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "<ipython-input-10-0b4a4a8ed1dd>:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] Batch [0/1407]\n",
            "Epoch [1/20] Batch [100/1407]\n",
            "Epoch [1/20] Batch [200/1407]\n",
            "Epoch [1/20] Batch [300/1407]\n",
            "Epoch [1/20] Batch [400/1407]\n",
            "Epoch [1/20] Batch [500/1407]\n",
            "Epoch [1/20] Batch [600/1407]\n",
            "Epoch [1/20] Batch [700/1407]\n",
            "Epoch [1/20] Batch [800/1407]\n",
            "Epoch [1/20] Batch [900/1407]\n",
            "Epoch [1/20] Batch [1000/1407]\n",
            "Epoch [1/20] Batch [1100/1407]\n",
            "Epoch [1/20] Batch [1200/1407]\n",
            "Epoch [1/20] Batch [1300/1407]\n",
            "Epoch [1/20] Batch [1400/1407]\n",
            "Epoch [1/20]\n",
            "Train Loss: 4.0511, Train Acc: 7.68%\n",
            "Val Loss: 3.7280, Val Acc: 12.08%\n",
            "Epoch [2/20] Batch [0/1407]\n",
            "Epoch [2/20] Batch [100/1407]\n",
            "Epoch [2/20] Batch [200/1407]\n",
            "Epoch [2/20] Batch [300/1407]\n",
            "Epoch [2/20] Batch [400/1407]\n",
            "Epoch [2/20] Batch [500/1407]\n",
            "Epoch [2/20] Batch [600/1407]\n",
            "Epoch [2/20] Batch [700/1407]\n",
            "Epoch [2/20] Batch [800/1407]\n",
            "Epoch [2/20] Batch [900/1407]\n",
            "Epoch [2/20] Batch [1000/1407]\n",
            "Epoch [2/20] Batch [1100/1407]\n",
            "Epoch [2/20] Batch [1200/1407]\n",
            "Epoch [2/20] Batch [1300/1407]\n",
            "Epoch [2/20] Batch [1400/1407]\n",
            "Epoch [2/20]\n",
            "Train Loss: 3.6642, Train Acc: 13.24%\n",
            "Val Loss: 3.4438, Val Acc: 17.58%\n",
            "Epoch [3/20] Batch [0/1407]\n",
            "Epoch [3/20] Batch [100/1407]\n",
            "Epoch [3/20] Batch [200/1407]\n",
            "Epoch [3/20] Batch [300/1407]\n",
            "Epoch [3/20] Batch [400/1407]\n",
            "Epoch [3/20] Batch [500/1407]\n",
            "Epoch [3/20] Batch [600/1407]\n",
            "Epoch [3/20] Batch [700/1407]\n",
            "Epoch [3/20] Batch [800/1407]\n",
            "Epoch [3/20] Batch [900/1407]\n",
            "Epoch [3/20] Batch [1000/1407]\n",
            "Epoch [3/20] Batch [1100/1407]\n",
            "Epoch [3/20] Batch [1200/1407]\n",
            "Epoch [3/20] Batch [1300/1407]\n",
            "Epoch [3/20] Batch [1400/1407]\n",
            "Epoch [3/20]\n",
            "Train Loss: 3.4044, Train Acc: 17.65%\n",
            "Val Loss: 3.2122, Val Acc: 20.80%\n",
            "Epoch [4/20] Batch [0/1407]\n",
            "Epoch [4/20] Batch [100/1407]\n",
            "Epoch [4/20] Batch [200/1407]\n",
            "Epoch [4/20] Batch [300/1407]\n",
            "Epoch [4/20] Batch [400/1407]\n",
            "Epoch [4/20] Batch [500/1407]\n",
            "Epoch [4/20] Batch [600/1407]\n",
            "Epoch [4/20] Batch [700/1407]\n",
            "Epoch [4/20] Batch [800/1407]\n",
            "Epoch [4/20] Batch [900/1407]\n",
            "Epoch [4/20] Batch [1000/1407]\n",
            "Epoch [4/20] Batch [1100/1407]\n",
            "Epoch [4/20] Batch [1200/1407]\n",
            "Epoch [4/20] Batch [1300/1407]\n",
            "Epoch [4/20] Batch [1400/1407]\n",
            "Epoch [4/20]\n",
            "Train Loss: 3.1892, Train Acc: 21.62%\n",
            "Val Loss: 2.9513, Val Acc: 26.24%\n",
            "Epoch [5/20] Batch [0/1407]\n",
            "Epoch [5/20] Batch [100/1407]\n",
            "Epoch [5/20] Batch [200/1407]\n",
            "Epoch [5/20] Batch [300/1407]\n",
            "Epoch [5/20] Batch [400/1407]\n",
            "Epoch [5/20] Batch [500/1407]\n",
            "Epoch [5/20] Batch [600/1407]\n",
            "Epoch [5/20] Batch [700/1407]\n",
            "Epoch [5/20] Batch [800/1407]\n",
            "Epoch [5/20] Batch [900/1407]\n",
            "Epoch [5/20] Batch [1000/1407]\n",
            "Epoch [5/20] Batch [1100/1407]\n",
            "Epoch [5/20] Batch [1200/1407]\n",
            "Epoch [5/20] Batch [1300/1407]\n",
            "Epoch [5/20] Batch [1400/1407]\n",
            "Epoch [5/20]\n",
            "Train Loss: 3.0350, Train Acc: 24.41%\n",
            "Val Loss: 2.8250, Val Acc: 28.28%\n",
            "Epoch [6/20] Batch [0/1407]\n",
            "Epoch [6/20] Batch [100/1407]\n",
            "Epoch [6/20] Batch [200/1407]\n",
            "Epoch [6/20] Batch [300/1407]\n",
            "Epoch [6/20] Batch [400/1407]\n",
            "Epoch [6/20] Batch [500/1407]\n",
            "Epoch [6/20] Batch [600/1407]\n",
            "Epoch [6/20] Batch [700/1407]\n",
            "Epoch [6/20] Batch [800/1407]\n",
            "Epoch [6/20] Batch [900/1407]\n",
            "Epoch [6/20] Batch [1000/1407]\n",
            "Epoch [6/20] Batch [1100/1407]\n",
            "Epoch [6/20] Batch [1200/1407]\n",
            "Epoch [6/20] Batch [1300/1407]\n",
            "Epoch [6/20] Batch [1400/1407]\n",
            "Epoch [6/20]\n",
            "Train Loss: 2.9161, Train Acc: 26.71%\n",
            "Val Loss: 2.7380, Val Acc: 31.14%\n",
            "Epoch [7/20] Batch [0/1407]\n",
            "Epoch [7/20] Batch [100/1407]\n",
            "Epoch [7/20] Batch [200/1407]\n",
            "Epoch [7/20] Batch [300/1407]\n",
            "Epoch [7/20] Batch [400/1407]\n",
            "Epoch [7/20] Batch [500/1407]\n",
            "Epoch [7/20] Batch [600/1407]\n",
            "Epoch [7/20] Batch [700/1407]\n",
            "Epoch [7/20] Batch [800/1407]\n",
            "Epoch [7/20] Batch [900/1407]\n",
            "Epoch [7/20] Batch [1000/1407]\n",
            "Epoch [7/20] Batch [1100/1407]\n",
            "Epoch [7/20] Batch [1200/1407]\n",
            "Epoch [7/20] Batch [1300/1407]\n",
            "Epoch [7/20] Batch [1400/1407]\n",
            "Epoch [7/20]\n",
            "Train Loss: 2.8131, Train Acc: 28.68%\n",
            "Val Loss: 2.6443, Val Acc: 32.60%\n",
            "Epoch [8/20] Batch [0/1407]\n",
            "Epoch [8/20] Batch [100/1407]\n",
            "Epoch [8/20] Batch [200/1407]\n",
            "Epoch [8/20] Batch [300/1407]\n",
            "Epoch [8/20] Batch [400/1407]\n",
            "Epoch [8/20] Batch [500/1407]\n",
            "Epoch [8/20] Batch [600/1407]\n",
            "Epoch [8/20] Batch [700/1407]\n",
            "Epoch [8/20] Batch [800/1407]\n",
            "Epoch [8/20] Batch [900/1407]\n",
            "Epoch [8/20] Batch [1000/1407]\n",
            "Epoch [8/20] Batch [1100/1407]\n",
            "Epoch [8/20] Batch [1200/1407]\n",
            "Epoch [8/20] Batch [1300/1407]\n",
            "Epoch [8/20] Batch [1400/1407]\n",
            "Epoch [8/20]\n",
            "Train Loss: 2.7435, Train Acc: 30.46%\n",
            "Val Loss: 2.6074, Val Acc: 33.46%\n",
            "Epoch [9/20] Batch [0/1407]\n",
            "Epoch [9/20] Batch [100/1407]\n",
            "Epoch [9/20] Batch [200/1407]\n",
            "Epoch [9/20] Batch [300/1407]\n",
            "Epoch [9/20] Batch [400/1407]\n",
            "Epoch [9/20] Batch [500/1407]\n",
            "Epoch [9/20] Batch [600/1407]\n",
            "Epoch [9/20] Batch [700/1407]\n",
            "Epoch [9/20] Batch [800/1407]\n",
            "Epoch [9/20] Batch [900/1407]\n",
            "Epoch [9/20] Batch [1000/1407]\n",
            "Epoch [9/20] Batch [1100/1407]\n",
            "Epoch [9/20] Batch [1200/1407]\n",
            "Epoch [9/20] Batch [1300/1407]\n",
            "Epoch [9/20] Batch [1400/1407]\n",
            "Epoch [9/20]\n",
            "Train Loss: 2.6681, Train Acc: 31.88%\n",
            "Val Loss: 2.5357, Val Acc: 34.46%\n",
            "Epoch [10/20] Batch [0/1407]\n",
            "Epoch [10/20] Batch [100/1407]\n",
            "Epoch [10/20] Batch [200/1407]\n",
            "Epoch [10/20] Batch [300/1407]\n",
            "Epoch [10/20] Batch [400/1407]\n",
            "Epoch [10/20] Batch [500/1407]\n",
            "Epoch [10/20] Batch [600/1407]\n",
            "Epoch [10/20] Batch [700/1407]\n",
            "Epoch [10/20] Batch [800/1407]\n",
            "Epoch [10/20] Batch [900/1407]\n",
            "Epoch [10/20] Batch [1000/1407]\n",
            "Epoch [10/20] Batch [1100/1407]\n",
            "Epoch [10/20] Batch [1200/1407]\n",
            "Epoch [10/20] Batch [1300/1407]\n",
            "Epoch [10/20] Batch [1400/1407]\n",
            "Epoch [10/20]\n",
            "Train Loss: 2.6098, Train Acc: 33.03%\n",
            "Val Loss: 2.4495, Val Acc: 37.38%\n",
            "Epoch [11/20] Batch [0/1407]\n",
            "Epoch [11/20] Batch [100/1407]\n",
            "Epoch [11/20] Batch [200/1407]\n",
            "Epoch [11/20] Batch [300/1407]\n",
            "Epoch [11/20] Batch [400/1407]\n",
            "Epoch [11/20] Batch [500/1407]\n",
            "Epoch [11/20] Batch [600/1407]\n",
            "Epoch [11/20] Batch [700/1407]\n",
            "Epoch [11/20] Batch [800/1407]\n",
            "Epoch [11/20] Batch [900/1407]\n",
            "Epoch [11/20] Batch [1000/1407]\n",
            "Epoch [11/20] Batch [1100/1407]\n",
            "Epoch [11/20] Batch [1200/1407]\n",
            "Epoch [11/20] Batch [1300/1407]\n",
            "Epoch [11/20] Batch [1400/1407]\n",
            "Epoch [11/20]\n",
            "Train Loss: 2.5618, Train Acc: 34.02%\n",
            "Val Loss: 2.4039, Val Acc: 38.08%\n",
            "Epoch [12/20] Batch [0/1407]\n",
            "Epoch [12/20] Batch [100/1407]\n",
            "Epoch [12/20] Batch [200/1407]\n",
            "Epoch [12/20] Batch [300/1407]\n",
            "Epoch [12/20] Batch [400/1407]\n",
            "Epoch [12/20] Batch [500/1407]\n",
            "Epoch [12/20] Batch [600/1407]\n",
            "Epoch [12/20] Batch [700/1407]\n",
            "Epoch [12/20] Batch [800/1407]\n",
            "Epoch [12/20] Batch [900/1407]\n",
            "Epoch [12/20] Batch [1000/1407]\n",
            "Epoch [12/20] Batch [1100/1407]\n",
            "Epoch [12/20] Batch [1200/1407]\n",
            "Epoch [12/20] Batch [1300/1407]\n",
            "Epoch [12/20] Batch [1400/1407]\n",
            "Epoch [12/20]\n",
            "Train Loss: 2.5128, Train Acc: 35.18%\n",
            "Val Loss: 2.3293, Val Acc: 38.52%\n",
            "Epoch [13/20] Batch [0/1407]\n",
            "Epoch [13/20] Batch [100/1407]\n",
            "Epoch [13/20] Batch [200/1407]\n",
            "Epoch [13/20] Batch [300/1407]\n",
            "Epoch [13/20] Batch [400/1407]\n",
            "Epoch [13/20] Batch [500/1407]\n",
            "Epoch [13/20] Batch [600/1407]\n",
            "Epoch [13/20] Batch [700/1407]\n",
            "Epoch [13/20] Batch [800/1407]\n",
            "Epoch [13/20] Batch [900/1407]\n",
            "Epoch [13/20] Batch [1000/1407]\n",
            "Epoch [13/20] Batch [1100/1407]\n",
            "Epoch [13/20] Batch [1200/1407]\n",
            "Epoch [13/20] Batch [1300/1407]\n",
            "Epoch [13/20] Batch [1400/1407]\n",
            "Epoch [13/20]\n",
            "Train Loss: 2.4747, Train Acc: 35.97%\n",
            "Val Loss: 2.2985, Val Acc: 40.18%\n",
            "Epoch [14/20] Batch [0/1407]\n",
            "Epoch [14/20] Batch [100/1407]\n",
            "Epoch [14/20] Batch [200/1407]\n",
            "Epoch [14/20] Batch [300/1407]\n",
            "Epoch [14/20] Batch [400/1407]\n",
            "Epoch [14/20] Batch [500/1407]\n",
            "Epoch [14/20] Batch [600/1407]\n",
            "Epoch [14/20] Batch [700/1407]\n",
            "Epoch [14/20] Batch [800/1407]\n",
            "Epoch [14/20] Batch [900/1407]\n",
            "Epoch [14/20] Batch [1000/1407]\n",
            "Epoch [14/20] Batch [1100/1407]\n",
            "Epoch [14/20] Batch [1200/1407]\n",
            "Epoch [14/20] Batch [1300/1407]\n",
            "Epoch [14/20] Batch [1400/1407]\n",
            "Epoch [14/20]\n",
            "Train Loss: 2.4337, Train Acc: 36.62%\n",
            "Val Loss: 2.2664, Val Acc: 40.34%\n",
            "Epoch [15/20] Batch [0/1407]\n",
            "Epoch [15/20] Batch [100/1407]\n",
            "Epoch [15/20] Batch [200/1407]\n",
            "Epoch [15/20] Batch [300/1407]\n",
            "Epoch [15/20] Batch [400/1407]\n",
            "Epoch [15/20] Batch [500/1407]\n",
            "Epoch [15/20] Batch [600/1407]\n",
            "Epoch [15/20] Batch [700/1407]\n",
            "Epoch [15/20] Batch [800/1407]\n",
            "Epoch [15/20] Batch [900/1407]\n",
            "Epoch [15/20] Batch [1000/1407]\n",
            "Epoch [15/20] Batch [1100/1407]\n",
            "Epoch [15/20] Batch [1200/1407]\n",
            "Epoch [15/20] Batch [1300/1407]\n",
            "Epoch [15/20] Batch [1400/1407]\n",
            "Epoch [15/20]\n",
            "Train Loss: 2.3990, Train Acc: 37.51%\n",
            "Val Loss: 2.2686, Val Acc: 40.42%\n",
            "Epoch [16/20] Batch [0/1407]\n",
            "Epoch [16/20] Batch [100/1407]\n",
            "Epoch [16/20] Batch [200/1407]\n",
            "Epoch [16/20] Batch [300/1407]\n",
            "Epoch [16/20] Batch [400/1407]\n",
            "Epoch [16/20] Batch [500/1407]\n",
            "Epoch [16/20] Batch [600/1407]\n",
            "Epoch [16/20] Batch [700/1407]\n",
            "Epoch [16/20] Batch [800/1407]\n",
            "Epoch [16/20] Batch [900/1407]\n",
            "Epoch [16/20] Batch [1000/1407]\n",
            "Epoch [16/20] Batch [1100/1407]\n",
            "Epoch [16/20] Batch [1200/1407]\n",
            "Epoch [16/20] Batch [1300/1407]\n",
            "Epoch [16/20] Batch [1400/1407]\n",
            "Epoch [16/20]\n",
            "Train Loss: 2.3701, Train Acc: 38.27%\n",
            "Val Loss: 2.2768, Val Acc: 39.82%\n",
            "Epoch [17/20] Batch [0/1407]\n",
            "Epoch [17/20] Batch [100/1407]\n",
            "Epoch [17/20] Batch [200/1407]\n",
            "Epoch [17/20] Batch [300/1407]\n",
            "Epoch [17/20] Batch [400/1407]\n",
            "Epoch [17/20] Batch [500/1407]\n",
            "Epoch [17/20] Batch [600/1407]\n",
            "Epoch [17/20] Batch [700/1407]\n",
            "Epoch [17/20] Batch [800/1407]\n",
            "Epoch [17/20] Batch [900/1407]\n",
            "Epoch [17/20] Batch [1000/1407]\n",
            "Epoch [17/20] Batch [1100/1407]\n",
            "Epoch [17/20] Batch [1200/1407]\n",
            "Epoch [17/20] Batch [1300/1407]\n",
            "Epoch [17/20] Batch [1400/1407]\n",
            "Epoch [17/20]\n",
            "Train Loss: 2.3410, Train Acc: 38.63%\n",
            "Val Loss: 2.2432, Val Acc: 41.26%\n",
            "Epoch [18/20] Batch [0/1407]\n",
            "Epoch [18/20] Batch [100/1407]\n",
            "Epoch [18/20] Batch [200/1407]\n",
            "Epoch [18/20] Batch [300/1407]\n",
            "Epoch [18/20] Batch [400/1407]\n",
            "Epoch [18/20] Batch [500/1407]\n",
            "Epoch [18/20] Batch [600/1407]\n",
            "Epoch [18/20] Batch [700/1407]\n",
            "Epoch [18/20] Batch [800/1407]\n",
            "Epoch [18/20] Batch [900/1407]\n",
            "Epoch [18/20] Batch [1000/1407]\n",
            "Epoch [18/20] Batch [1100/1407]\n",
            "Epoch [18/20] Batch [1200/1407]\n",
            "Epoch [18/20] Batch [1300/1407]\n",
            "Epoch [18/20] Batch [1400/1407]\n",
            "Epoch [18/20]\n",
            "Train Loss: 2.3053, Train Acc: 39.57%\n",
            "Val Loss: 2.2213, Val Acc: 41.98%\n",
            "Epoch [19/20] Batch [0/1407]\n",
            "Epoch [19/20] Batch [100/1407]\n",
            "Epoch [19/20] Batch [200/1407]\n",
            "Epoch [19/20] Batch [300/1407]\n",
            "Epoch [19/20] Batch [400/1407]\n",
            "Epoch [19/20] Batch [500/1407]\n",
            "Epoch [19/20] Batch [600/1407]\n",
            "Epoch [19/20] Batch [700/1407]\n",
            "Epoch [19/20] Batch [800/1407]\n",
            "Epoch [19/20] Batch [900/1407]\n",
            "Epoch [19/20] Batch [1000/1407]\n",
            "Epoch [19/20] Batch [1100/1407]\n",
            "Epoch [19/20] Batch [1200/1407]\n",
            "Epoch [19/20] Batch [1300/1407]\n",
            "Epoch [19/20] Batch [1400/1407]\n",
            "Epoch [19/20]\n",
            "Train Loss: 2.2878, Train Acc: 39.84%\n",
            "Val Loss: 2.1773, Val Acc: 43.34%\n",
            "Epoch [20/20] Batch [0/1407]\n",
            "Epoch [20/20] Batch [100/1407]\n",
            "Epoch [20/20] Batch [200/1407]\n",
            "Epoch [20/20] Batch [300/1407]\n",
            "Epoch [20/20] Batch [400/1407]\n",
            "Epoch [20/20] Batch [500/1407]\n",
            "Epoch [20/20] Batch [600/1407]\n",
            "Epoch [20/20] Batch [700/1407]\n",
            "Epoch [20/20] Batch [800/1407]\n",
            "Epoch [20/20] Batch [900/1407]\n",
            "Epoch [20/20] Batch [1000/1407]\n",
            "Epoch [20/20] Batch [1100/1407]\n",
            "Epoch [20/20] Batch [1200/1407]\n",
            "Epoch [20/20] Batch [1300/1407]\n",
            "Epoch [20/20] Batch [1400/1407]\n",
            "Epoch [20/20]\n",
            "Train Loss: 2.2669, Train Acc: 40.55%\n",
            "Val Loss: 2.1560, Val Acc: 43.00%\n",
            "\n",
            "Generating classification report...\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        apple     0.6667    0.7800    0.7189       100\n",
            "aquarium_fish     0.4083    0.6900    0.5130       100\n",
            "         baby     0.3976    0.3300    0.3607       100\n",
            "         bear     0.3088    0.2100    0.2500       100\n",
            "       beaver     0.2710    0.2900    0.2802       100\n",
            "          bed     0.5467    0.4100    0.4686       100\n",
            "          bee     0.5268    0.5900    0.5566       100\n",
            "       beetle     0.5408    0.5300    0.5354       100\n",
            "      bicycle     0.6923    0.5400    0.6067       100\n",
            "       bottle     0.6962    0.5500    0.6145       100\n",
            "         bowl     0.4194    0.2600    0.3210       100\n",
            "          boy     0.4769    0.3100    0.3758       100\n",
            "       bridge     0.7333    0.4400    0.5500       100\n",
            "          bus     0.4234    0.4700    0.4455       100\n",
            "    butterfly     0.4343    0.4300    0.4322       100\n",
            "        camel     0.4848    0.3200    0.3855       100\n",
            "          can     0.5696    0.4500    0.5028       100\n",
            "       castle     0.5985    0.7900    0.6810       100\n",
            "  caterpillar     0.4412    0.4500    0.4455       100\n",
            "       cattle     0.6275    0.3200    0.4238       100\n",
            "        chair     0.8182    0.7200    0.7660       100\n",
            "   chimpanzee     0.5507    0.7600    0.6387       100\n",
            "        clock     0.3256    0.5600    0.4118       100\n",
            "        cloud     0.6854    0.6100    0.6455       100\n",
            "    cockroach     0.5442    0.8000    0.6478       100\n",
            "        couch     0.4717    0.2500    0.3268       100\n",
            "         crab     0.4444    0.4000    0.4211       100\n",
            "    crocodile     0.3529    0.4200    0.3836       100\n",
            "          cup     0.8243    0.6100    0.7011       100\n",
            "     dinosaur     0.5467    0.4100    0.4686       100\n",
            "      dolphin     0.3952    0.4900    0.4375       100\n",
            "     elephant     0.6230    0.3800    0.4720       100\n",
            "     flatfish     0.5616    0.4100    0.4740       100\n",
            "       forest     0.5326    0.4900    0.5104       100\n",
            "          fox     0.4483    0.5200    0.4815       100\n",
            "         girl     0.4242    0.1400    0.2105       100\n",
            "      hamster     0.4836    0.5900    0.5315       100\n",
            "        house     0.5128    0.4000    0.4494       100\n",
            "     kangaroo     0.4250    0.1700    0.2429       100\n",
            "     keyboard     0.5403    0.6700    0.5982       100\n",
            "         lamp     0.5616    0.4100    0.4740       100\n",
            "   lawn_mower     0.6281    0.7600    0.6878       100\n",
            "      leopard     0.5368    0.5100    0.5231       100\n",
            "         lion     0.4129    0.6400    0.5020       100\n",
            "       lizard     0.2766    0.1300    0.1769       100\n",
            "      lobster     0.3109    0.3700    0.3379       100\n",
            "          man     0.3486    0.3800    0.3636       100\n",
            "   maple_tree     0.6429    0.4500    0.5294       100\n",
            "   motorcycle     0.6338    0.9000    0.7438       100\n",
            "     mountain     0.5745    0.8100    0.6722       100\n",
            "        mouse     0.2706    0.2300    0.2486       100\n",
            "     mushroom     0.4215    0.5100    0.4615       100\n",
            "     oak_tree     0.4579    0.8700    0.6000       100\n",
            "       orange     0.5269    0.8800    0.6592       100\n",
            "       orchid     0.4088    0.7400    0.5267       100\n",
            "        otter     0.1875    0.0300    0.0517       100\n",
            "    palm_tree     0.7882    0.6700    0.7243       100\n",
            "         pear     0.6623    0.5100    0.5763       100\n",
            " pickup_truck     0.6146    0.5900    0.6020       100\n",
            "    pine_tree     0.5506    0.4900    0.5185       100\n",
            "        plain     0.6829    0.8400    0.7534       100\n",
            "        plate     0.4949    0.4900    0.4925       100\n",
            "        poppy     0.5326    0.4900    0.5104       100\n",
            "    porcupine     0.6212    0.4100    0.4940       100\n",
            "       possum     0.2750    0.1100    0.1571       100\n",
            "       rabbit     0.4524    0.1900    0.2676       100\n",
            "      raccoon     0.3750    0.4800    0.4211       100\n",
            "          ray     0.4286    0.2400    0.3077       100\n",
            "         road     0.7500    0.9000    0.8182       100\n",
            "       rocket     0.6465    0.6400    0.6432       100\n",
            "         rose     0.4414    0.6400    0.5224       100\n",
            "          sea     0.6559    0.6100    0.6321       100\n",
            "         seal     0.3913    0.0900    0.1463       100\n",
            "        shark     0.5082    0.3100    0.3851       100\n",
            "        shrew     0.1792    0.5000    0.2639       100\n",
            "        skunk     0.5608    0.8300    0.6694       100\n",
            "   skyscraper     0.8375    0.6700    0.7444       100\n",
            "        snail     0.2398    0.5300    0.3302       100\n",
            "        snake     0.3083    0.4100    0.3519       100\n",
            "       spider     0.5169    0.4600    0.4868       100\n",
            "     squirrel     0.2243    0.2400    0.2319       100\n",
            "    streetcar     0.5275    0.4800    0.5026       100\n",
            "    sunflower     0.6391    0.8500    0.7296       100\n",
            " sweet_pepper     0.4444    0.3600    0.3978       100\n",
            "        table     0.5349    0.4600    0.4946       100\n",
            "         tank     0.6711    0.5100    0.5795       100\n",
            "    telephone     0.6047    0.5200    0.5591       100\n",
            "   television     0.7500    0.5700    0.6477       100\n",
            "        tiger     0.6962    0.5500    0.6145       100\n",
            "      tractor     0.5078    0.6500    0.5702       100\n",
            "        train     0.4954    0.5400    0.5167       100\n",
            "        trout     0.7000    0.5600    0.6222       100\n",
            "        tulip     0.3596    0.4100    0.3832       100\n",
            "       turtle     0.2164    0.3700    0.2731       100\n",
            "     wardrobe     0.8462    0.7700    0.8063       100\n",
            "        whale     0.6949    0.4100    0.5157       100\n",
            "  willow_tree     0.4727    0.2600    0.3355       100\n",
            "         wolf     0.3182    0.5600    0.4058       100\n",
            "        woman     0.3333    0.2400    0.2791       100\n",
            "         worm     0.5699    0.5300    0.5492       100\n",
            "\n",
            "     accuracy                         0.4952     10000\n",
            "    macro avg     0.5110    0.4952    0.4868     10000\n",
            " weighted avg     0.5110    0.4952    0.4868     10000\n",
            "\n",
            "\n",
            "Report saved to /content/drive/MyDrive/ML Assignment 3/cifar100_reports\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train/accuracy</td><td>▁▂▃▄▅▅▅▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/loss</td><td>█▆▅▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val/accuracy</td><td>▁▂▃▄▅▅▆▆▆▇▇▇▇▇▇▇████</td></tr><tr><td>val/loss</td><td>█▇▆▅▄▄▃▃▃▂▂▂▂▁▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train/accuracy</td><td>40.55111</td></tr><tr><td>train/loss</td><td>2.26691</td></tr><tr><td>val/accuracy</td><td>43</td></tr><tr><td>val/loss</td><td>2.15602</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">misty-butterfly-12</strong> at: <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/4k4ayn5c' target=\"_blank\">https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/4k4ayn5c</a><br> View project at: <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn' target=\"_blank\">https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn</a><br>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250322_203406-4k4ayn5c/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250322_205530-jp5ta4an</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/jp5ta4an' target=\"_blank\">tough-yogurt-13</a></strong> to <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn' target=\"_blank\">https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/jp5ta4an' target=\"_blank\">https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/jp5ta4an</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with config: {'lr': 0.0001, 'batch_size': 256, 'optimizer': 'adam', 'dropout': 0.7, 'epochs': 20}\n",
            "Generating random image samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-0b4a4a8ed1dd>:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "<ipython-input-10-0b4a4a8ed1dd>:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] Batch [0/176]\n",
            "Epoch [1/20] Batch [100/176]\n",
            "Epoch [1/20]\n",
            "Train Loss: 4.6386, Train Acc: 1.50%\n",
            "Val Loss: 4.5229, Val Acc: 2.54%\n",
            "Epoch [2/20] Batch [0/176]\n",
            "Epoch [2/20] Batch [100/176]\n",
            "Epoch [2/20]\n",
            "Train Loss: 4.4885, Train Acc: 2.62%\n",
            "Val Loss: 4.3691, Val Acc: 4.50%\n",
            "Epoch [3/20] Batch [0/176]\n",
            "Epoch [3/20] Batch [100/176]\n",
            "Epoch [3/20]\n",
            "Train Loss: 4.3795, Train Acc: 3.75%\n",
            "Val Loss: 4.2136, Val Acc: 7.14%\n",
            "Epoch [4/20] Batch [0/176]\n",
            "Epoch [4/20] Batch [100/176]\n",
            "Epoch [4/20]\n",
            "Train Loss: 4.2553, Train Acc: 4.96%\n",
            "Val Loss: 4.0463, Val Acc: 9.34%\n",
            "Epoch [5/20] Batch [0/176]\n",
            "Epoch [5/20] Batch [100/176]\n",
            "Epoch [5/20]\n",
            "Train Loss: 4.1539, Train Acc: 6.21%\n",
            "Val Loss: 3.9552, Val Acc: 11.08%\n",
            "Epoch [6/20] Batch [0/176]\n",
            "Epoch [6/20] Batch [100/176]\n",
            "Epoch [6/20]\n",
            "Train Loss: 4.0679, Train Acc: 7.15%\n",
            "Val Loss: 3.8644, Val Acc: 11.60%\n",
            "Epoch [7/20] Batch [0/176]\n",
            "Epoch [7/20] Batch [100/176]\n",
            "Epoch [7/20]\n",
            "Train Loss: 3.9904, Train Acc: 8.26%\n",
            "Val Loss: 3.7857, Val Acc: 13.10%\n",
            "Epoch [8/20] Batch [0/176]\n",
            "Epoch [8/20] Batch [100/176]\n",
            "Epoch [8/20]\n",
            "Train Loss: 3.9320, Train Acc: 8.99%\n",
            "Val Loss: 3.7226, Val Acc: 13.22%\n",
            "Epoch [9/20] Batch [0/176]\n",
            "Epoch [9/20] Batch [100/176]\n",
            "Epoch [9/20]\n",
            "Train Loss: 3.8843, Train Acc: 9.76%\n",
            "Val Loss: 3.6829, Val Acc: 14.18%\n",
            "Epoch [10/20] Batch [0/176]\n",
            "Epoch [10/20] Batch [100/176]\n",
            "Epoch [10/20]\n",
            "Train Loss: 3.8331, Train Acc: 10.41%\n",
            "Val Loss: 3.6338, Val Acc: 15.18%\n",
            "Epoch [11/20] Batch [0/176]\n",
            "Epoch [11/20] Batch [100/176]\n",
            "Epoch [11/20]\n",
            "Train Loss: 3.7934, Train Acc: 11.14%\n",
            "Val Loss: 3.5959, Val Acc: 15.72%\n",
            "Epoch [12/20] Batch [0/176]\n",
            "Epoch [12/20] Batch [100/176]\n",
            "Epoch [12/20]\n",
            "Train Loss: 3.7547, Train Acc: 11.95%\n",
            "Val Loss: 3.5320, Val Acc: 16.06%\n",
            "Epoch [13/20] Batch [0/176]\n",
            "Epoch [13/20] Batch [100/176]\n",
            "Epoch [13/20]\n",
            "Train Loss: 3.7129, Train Acc: 12.43%\n",
            "Val Loss: 3.5116, Val Acc: 17.14%\n",
            "Epoch [14/20] Batch [0/176]\n",
            "Epoch [14/20] Batch [100/176]\n",
            "Epoch [14/20]\n",
            "Train Loss: 3.6786, Train Acc: 12.99%\n",
            "Val Loss: 3.4627, Val Acc: 18.12%\n",
            "Epoch [15/20] Batch [0/176]\n",
            "Epoch [15/20] Batch [100/176]\n",
            "Epoch [15/20]\n",
            "Train Loss: 3.6344, Train Acc: 13.65%\n",
            "Val Loss: 3.4195, Val Acc: 18.62%\n",
            "Epoch [16/20] Batch [0/176]\n",
            "Epoch [16/20] Batch [100/176]\n",
            "Epoch [16/20]\n",
            "Train Loss: 3.6182, Train Acc: 14.14%\n",
            "Val Loss: 3.3919, Val Acc: 19.42%\n",
            "Epoch [17/20] Batch [0/176]\n",
            "Epoch [17/20] Batch [100/176]\n",
            "Epoch [17/20]\n",
            "Train Loss: 3.5815, Train Acc: 14.53%\n",
            "Val Loss: 3.3560, Val Acc: 20.36%\n",
            "Epoch [18/20] Batch [0/176]\n",
            "Epoch [18/20] Batch [100/176]\n",
            "Epoch [18/20]\n",
            "Train Loss: 3.5549, Train Acc: 14.98%\n",
            "Val Loss: 3.3125, Val Acc: 20.20%\n",
            "Epoch [19/20] Batch [0/176]\n",
            "Epoch [19/20] Batch [100/176]\n",
            "Epoch [19/20]\n",
            "Train Loss: 3.5302, Train Acc: 15.71%\n",
            "Val Loss: 3.2837, Val Acc: 21.56%\n",
            "Epoch [20/20] Batch [0/176]\n",
            "Epoch [20/20] Batch [100/176]\n",
            "Epoch [20/20]\n",
            "Train Loss: 3.5017, Train Acc: 16.03%\n",
            "Val Loss: 3.2802, Val Acc: 21.06%\n",
            "\n",
            "Generating classification report...\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        apple     0.3452    0.6800    0.4579       100\n",
            "aquarium_fish     0.2404    0.2500    0.2451       100\n",
            "         baby     0.2167    0.1300    0.1625       100\n",
            "         bear     0.1149    0.1000    0.1070       100\n",
            "       beaver     0.1765    0.0300    0.0513       100\n",
            "          bed     0.3000    0.0600    0.1000       100\n",
            "          bee     0.3750    0.1800    0.2432       100\n",
            "       beetle     0.1798    0.1600    0.1693       100\n",
            "      bicycle     0.3103    0.2700    0.2888       100\n",
            "       bottle     0.2598    0.3300    0.2907       100\n",
            "         bowl     0.0000    0.0000    0.0000       100\n",
            "          boy     0.1321    0.0700    0.0915       100\n",
            "       bridge     0.3023    0.2600    0.2796       100\n",
            "          bus     0.2500    0.2500    0.2500       100\n",
            "    butterfly     0.1931    0.2800    0.2286       100\n",
            "        camel     0.1714    0.1200    0.1412       100\n",
            "          can     0.2105    0.0400    0.0672       100\n",
            "       castle     0.3111    0.5600    0.4000       100\n",
            "  caterpillar     0.1944    0.4200    0.2658       100\n",
            "       cattle     0.3333    0.0700    0.1157       100\n",
            "        chair     0.2500    0.5100    0.3355       100\n",
            "   chimpanzee     0.1927    0.6300    0.2951       100\n",
            "        clock     0.3333    0.0600    0.1017       100\n",
            "        cloud     0.3822    0.6000    0.4669       100\n",
            "    cockroach     0.1832    0.5900    0.2796       100\n",
            "        couch     0.2917    0.0700    0.1129       100\n",
            "         crab     0.2000    0.0400    0.0667       100\n",
            "    crocodile     0.2667    0.0800    0.1231       100\n",
            "          cup     0.2239    0.1500    0.1796       100\n",
            "     dinosaur     0.2167    0.1300    0.1625       100\n",
            "      dolphin     0.1925    0.3100    0.2375       100\n",
            "     elephant     0.2449    0.2400    0.2424       100\n",
            "     flatfish     0.4286    0.0300    0.0561       100\n",
            "       forest     0.3256    0.2800    0.3011       100\n",
            "          fox     0.1429    0.0200    0.0351       100\n",
            "         girl     0.1562    0.2000    0.1754       100\n",
            "      hamster     0.1566    0.4400    0.2310       100\n",
            "        house     0.3448    0.2000    0.2532       100\n",
            "     kangaroo     0.1353    0.2300    0.1704       100\n",
            "     keyboard     0.2308    0.0600    0.0952       100\n",
            "         lamp     0.3333    0.0200    0.0377       100\n",
            "   lawn_mower     0.5618    0.5000    0.5291       100\n",
            "      leopard     0.2286    0.4000    0.2909       100\n",
            "         lion     0.1583    0.4400    0.2328       100\n",
            "       lizard     0.1364    0.0300    0.0492       100\n",
            "      lobster     0.1724    0.1000    0.1266       100\n",
            "          man     0.2222    0.2200    0.2211       100\n",
            "   maple_tree     0.4861    0.3500    0.4070       100\n",
            "   motorcycle     0.2523    0.5600    0.3478       100\n",
            "     mountain     0.3373    0.2800    0.3060       100\n",
            "        mouse     0.1304    0.0300    0.0488       100\n",
            "     mushroom     0.1429    0.0800    0.1026       100\n",
            "     oak_tree     0.3451    0.8800    0.4958       100\n",
            "       orange     0.3393    0.7600    0.4691       100\n",
            "       orchid     0.3456    0.4700    0.3983       100\n",
            "        otter     0.0833    0.0100    0.0179       100\n",
            "    palm_tree     0.2619    0.2200    0.2391       100\n",
            "         pear     0.2821    0.2200    0.2472       100\n",
            " pickup_truck     0.2625    0.2100    0.2333       100\n",
            "    pine_tree     0.1042    0.0500    0.0676       100\n",
            "        plain     0.5538    0.7200    0.6261       100\n",
            "        plate     0.1940    0.1300    0.1557       100\n",
            "        poppy     0.2802    0.5100    0.3617       100\n",
            "    porcupine     0.1685    0.1500    0.1587       100\n",
            "       possum     0.1126    0.1700    0.1355       100\n",
            "       rabbit     0.1270    0.0800    0.0982       100\n",
            "      raccoon     0.0976    0.1200    0.1076       100\n",
            "          ray     0.2353    0.3600    0.2846       100\n",
            "         road     0.5043    0.5900    0.5438       100\n",
            "       rocket     0.3400    0.3400    0.3400       100\n",
            "         rose     0.3121    0.4400    0.3651       100\n",
            "          sea     0.5733    0.4300    0.4914       100\n",
            "         seal     0.0000    0.0000    0.0000       100\n",
            "        shark     0.2276    0.2800    0.2511       100\n",
            "        shrew     0.1081    0.0400    0.0584       100\n",
            "        skunk     0.2465    0.5300    0.3365       100\n",
            "   skyscraper     0.3243    0.6000    0.4211       100\n",
            "        snail     0.0526    0.0100    0.0168       100\n",
            "        snake     0.0667    0.0400    0.0500       100\n",
            "       spider     0.3333    0.0400    0.0714       100\n",
            "     squirrel     0.1471    0.0500    0.0746       100\n",
            "    streetcar     0.3433    0.2300    0.2754       100\n",
            "    sunflower     0.4600    0.6900    0.5520       100\n",
            " sweet_pepper     0.1765    0.0600    0.0896       100\n",
            "        table     0.1739    0.0400    0.0650       100\n",
            "         tank     0.3373    0.2800    0.3060       100\n",
            "    telephone     0.2100    0.4600    0.2884       100\n",
            "   television     0.5500    0.1100    0.1833       100\n",
            "        tiger     0.1823    0.3700    0.2442       100\n",
            "      tractor     0.3300    0.3300    0.3300       100\n",
            "        train     0.3421    0.1300    0.1884       100\n",
            "        trout     0.2500    0.0400    0.0690       100\n",
            "        tulip     0.1731    0.0900    0.1184       100\n",
            "       turtle     0.2258    0.0700    0.1069       100\n",
            "     wardrobe     0.2684    0.5100    0.3517       100\n",
            "        whale     0.2167    0.3900    0.2786       100\n",
            "  willow_tree     0.1927    0.2100    0.2010       100\n",
            "         wolf     0.1562    0.2500    0.1923       100\n",
            "        woman     0.1299    0.2000    0.1575       100\n",
            "         worm     0.0000    0.0000    0.0000       100\n",
            "\n",
            "     accuracy                         0.2505     10000\n",
            "    macro avg     0.2442    0.2505    0.2169     10000\n",
            " weighted avg     0.2442    0.2505    0.2169     10000\n",
            "\n",
            "\n",
            "Report saved to /content/drive/MyDrive/ML Assignment 3/cifar100_reports\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train/accuracy</td><td>▁▂▂▃▃▄▄▅▅▅▆▆▆▇▇▇▇▇██</td></tr><tr><td>train/loss</td><td>█▇▆▆▅▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>val/accuracy</td><td>▁▂▃▄▄▄▅▅▅▆▆▆▆▇▇▇█▇██</td></tr><tr><td>val/loss</td><td>█▇▆▅▅▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train/accuracy</td><td>16.02889</td></tr><tr><td>train/loss</td><td>3.50173</td></tr><tr><td>val/accuracy</td><td>21.06</td></tr><tr><td>val/loss</td><td>3.28018</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">tough-yogurt-13</strong> at: <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/jp5ta4an' target=\"_blank\">https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/jp5ta4an</a><br> View project at: <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn' target=\"_blank\">https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn</a><br>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250322_205530-jp5ta4an/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250322_211512-durouo2q</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/durouo2q' target=\"_blank\">dazzling-wood-14</a></strong> to <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn' target=\"_blank\">https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/durouo2q' target=\"_blank\">https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/durouo2q</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with config: {'lr': 0.005, 'batch_size': 128, 'optimizer': 'sgd', 'dropout': 0.4, 'epochs': 20}\n",
            "Generating random image samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-0b4a4a8ed1dd>:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "<ipython-input-10-0b4a4a8ed1dd>:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] Batch [0/352]\n",
            "Epoch [1/20] Batch [100/352]\n",
            "Epoch [1/20] Batch [200/352]\n",
            "Epoch [1/20] Batch [300/352]\n",
            "Epoch [1/20]\n",
            "Train Loss: 4.1687, Train Acc: 6.58%\n",
            "Val Loss: 3.7753, Val Acc: 12.74%\n",
            "Epoch [2/20] Batch [0/352]\n",
            "Epoch [2/20] Batch [100/352]\n",
            "Epoch [2/20] Batch [200/352]\n",
            "Epoch [2/20] Batch [300/352]\n",
            "Epoch [2/20]\n",
            "Train Loss: 3.7256, Train Acc: 12.34%\n",
            "Val Loss: 3.5320, Val Acc: 16.40%\n",
            "Epoch [3/20] Batch [0/352]\n",
            "Epoch [3/20] Batch [100/352]\n",
            "Epoch [3/20] Batch [200/352]\n",
            "Epoch [3/20] Batch [300/352]\n",
            "Epoch [3/20]\n",
            "Train Loss: 3.4827, Train Acc: 16.11%\n",
            "Val Loss: 3.3211, Val Acc: 19.96%\n",
            "Epoch [4/20] Batch [0/352]\n",
            "Epoch [4/20] Batch [100/352]\n",
            "Epoch [4/20] Batch [200/352]\n",
            "Epoch [4/20] Batch [300/352]\n",
            "Epoch [4/20]\n",
            "Train Loss: 3.3257, Train Acc: 18.80%\n",
            "Val Loss: 3.1264, Val Acc: 23.14%\n",
            "Epoch [5/20] Batch [0/352]\n",
            "Epoch [5/20] Batch [100/352]\n",
            "Epoch [5/20] Batch [200/352]\n",
            "Epoch [5/20] Batch [300/352]\n",
            "Epoch [5/20]\n",
            "Train Loss: 3.1914, Train Acc: 21.57%\n",
            "Val Loss: 3.0712, Val Acc: 24.72%\n",
            "Epoch [6/20] Batch [0/352]\n",
            "Epoch [6/20] Batch [100/352]\n",
            "Epoch [6/20] Batch [200/352]\n",
            "Epoch [6/20] Batch [300/352]\n",
            "Epoch [6/20]\n",
            "Train Loss: 3.0921, Train Acc: 23.15%\n",
            "Val Loss: 2.9172, Val Acc: 27.76%\n",
            "Epoch [7/20] Batch [0/352]\n",
            "Epoch [7/20] Batch [100/352]\n",
            "Epoch [7/20] Batch [200/352]\n",
            "Epoch [7/20] Batch [300/352]\n",
            "Epoch [7/20]\n",
            "Train Loss: 3.0078, Train Acc: 24.82%\n",
            "Val Loss: 2.9410, Val Acc: 27.48%\n",
            "Epoch [8/20] Batch [0/352]\n",
            "Epoch [8/20] Batch [100/352]\n",
            "Epoch [8/20] Batch [200/352]\n",
            "Epoch [8/20] Batch [300/352]\n",
            "Epoch [8/20]\n",
            "Train Loss: 2.9448, Train Acc: 26.06%\n",
            "Val Loss: 2.7992, Val Acc: 30.62%\n",
            "Epoch [9/20] Batch [0/352]\n",
            "Epoch [9/20] Batch [100/352]\n",
            "Epoch [9/20] Batch [200/352]\n",
            "Epoch [9/20] Batch [300/352]\n",
            "Epoch [9/20]\n",
            "Train Loss: 2.8821, Train Acc: 27.24%\n",
            "Val Loss: 2.8040, Val Acc: 29.44%\n",
            "Epoch [10/20] Batch [0/352]\n",
            "Epoch [10/20] Batch [100/352]\n",
            "Epoch [10/20] Batch [200/352]\n",
            "Epoch [10/20] Batch [300/352]\n",
            "Epoch [10/20]\n",
            "Train Loss: 2.8265, Train Acc: 28.50%\n",
            "Val Loss: 2.8168, Val Acc: 29.66%\n",
            "Epoch [11/20] Batch [0/352]\n",
            "Epoch [11/20] Batch [100/352]\n",
            "Epoch [11/20] Batch [200/352]\n",
            "Epoch [11/20] Batch [300/352]\n",
            "Epoch [11/20]\n",
            "Train Loss: 2.7868, Train Acc: 29.19%\n",
            "Val Loss: 2.6932, Val Acc: 32.44%\n",
            "Epoch [12/20] Batch [0/352]\n",
            "Epoch [12/20] Batch [100/352]\n",
            "Epoch [12/20] Batch [200/352]\n",
            "Epoch [12/20] Batch [300/352]\n",
            "Epoch [12/20]\n",
            "Train Loss: 2.7378, Train Acc: 30.34%\n",
            "Val Loss: 2.6260, Val Acc: 33.28%\n",
            "Epoch [13/20] Batch [0/352]\n",
            "Epoch [13/20] Batch [100/352]\n",
            "Epoch [13/20] Batch [200/352]\n",
            "Epoch [13/20] Batch [300/352]\n",
            "Epoch [13/20]\n",
            "Train Loss: 2.7051, Train Acc: 30.72%\n",
            "Val Loss: 2.6464, Val Acc: 32.82%\n",
            "Epoch [14/20] Batch [0/352]\n",
            "Epoch [14/20] Batch [100/352]\n",
            "Epoch [14/20] Batch [200/352]\n",
            "Epoch [14/20] Batch [300/352]\n",
            "Epoch [14/20]\n",
            "Train Loss: 2.6601, Train Acc: 31.54%\n",
            "Val Loss: 2.5604, Val Acc: 35.30%\n",
            "Epoch [15/20] Batch [0/352]\n",
            "Epoch [15/20] Batch [100/352]\n",
            "Epoch [15/20] Batch [200/352]\n",
            "Epoch [15/20] Batch [300/352]\n",
            "Epoch [15/20]\n",
            "Train Loss: 2.6265, Train Acc: 32.35%\n",
            "Val Loss: 2.5108, Val Acc: 35.24%\n",
            "Epoch [16/20] Batch [0/352]\n",
            "Epoch [16/20] Batch [100/352]\n",
            "Epoch [16/20] Batch [200/352]\n",
            "Epoch [16/20] Batch [300/352]\n",
            "Epoch [16/20]\n",
            "Train Loss: 2.6014, Train Acc: 32.96%\n",
            "Val Loss: 2.4495, Val Acc: 37.36%\n",
            "Epoch [17/20] Batch [0/352]\n",
            "Epoch [17/20] Batch [100/352]\n",
            "Epoch [17/20] Batch [200/352]\n",
            "Epoch [17/20] Batch [300/352]\n",
            "Epoch [17/20]\n",
            "Train Loss: 2.5710, Train Acc: 33.78%\n",
            "Val Loss: 2.4404, Val Acc: 37.56%\n",
            "Epoch [18/20] Batch [0/352]\n",
            "Epoch [18/20] Batch [100/352]\n",
            "Epoch [18/20] Batch [200/352]\n",
            "Epoch [18/20] Batch [300/352]\n",
            "Epoch [18/20]\n",
            "Train Loss: 2.5474, Train Acc: 34.18%\n",
            "Val Loss: 2.4587, Val Acc: 36.78%\n",
            "Epoch [19/20] Batch [0/352]\n",
            "Epoch [19/20] Batch [100/352]\n",
            "Epoch [19/20] Batch [200/352]\n",
            "Epoch [19/20] Batch [300/352]\n",
            "Epoch [19/20]\n",
            "Train Loss: 2.5223, Train Acc: 34.51%\n",
            "Val Loss: 2.4039, Val Acc: 38.00%\n",
            "Epoch [20/20] Batch [0/352]\n",
            "Epoch [20/20] Batch [100/352]\n",
            "Epoch [20/20] Batch [200/352]\n",
            "Epoch [20/20] Batch [300/352]\n",
            "Epoch [20/20]\n",
            "Train Loss: 2.5036, Train Acc: 35.11%\n",
            "Val Loss: 2.3355, Val Acc: 39.94%\n",
            "\n",
            "Generating classification report...\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        apple     0.6250    0.8000    0.7018       100\n",
            "aquarium_fish     0.5169    0.6100    0.5596       100\n",
            "         baby     0.3924    0.3100    0.3464       100\n",
            "         bear     0.2500    0.1200    0.1622       100\n",
            "       beaver     0.3469    0.1700    0.2282       100\n",
            "          bed     0.4066    0.3700    0.3874       100\n",
            "          bee     0.3892    0.6500    0.4869       100\n",
            "       beetle     0.5385    0.4200    0.4719       100\n",
            "      bicycle     0.5802    0.4700    0.5193       100\n",
            "       bottle     0.4662    0.6200    0.5322       100\n",
            "         bowl     0.3407    0.3100    0.3246       100\n",
            "          boy     0.4314    0.2200    0.2914       100\n",
            "       bridge     0.6452    0.4000    0.4938       100\n",
            "          bus     0.3036    0.3400    0.3208       100\n",
            "    butterfly     0.4133    0.3100    0.3543       100\n",
            "        camel     0.3253    0.2700    0.2951       100\n",
            "          can     0.6316    0.3600    0.4586       100\n",
            "       castle     0.5882    0.6000    0.5941       100\n",
            "  caterpillar     0.3964    0.4400    0.4171       100\n",
            "       cattle     0.4918    0.3000    0.3727       100\n",
            "        chair     0.7553    0.7100    0.7320       100\n",
            "   chimpanzee     0.4000    0.8000    0.5333       100\n",
            "        clock     0.3396    0.3600    0.3495       100\n",
            "        cloud     0.6095    0.6400    0.6244       100\n",
            "    cockroach     0.4718    0.6700    0.5537       100\n",
            "        couch     0.5385    0.2100    0.3022       100\n",
            "         crab     0.5294    0.3600    0.4286       100\n",
            "    crocodile     0.2977    0.3900    0.3377       100\n",
            "          cup     0.5980    0.6100    0.6040       100\n",
            "     dinosaur     0.4355    0.2700    0.3333       100\n",
            "      dolphin     0.3219    0.4700    0.3821       100\n",
            "     elephant     0.4123    0.4700    0.4393       100\n",
            "     flatfish     0.5143    0.3600    0.4235       100\n",
            "       forest     0.5645    0.3500    0.4321       100\n",
            "          fox     0.3380    0.4800    0.3967       100\n",
            "         girl     0.2985    0.2000    0.2395       100\n",
            "      hamster     0.4000    0.4800    0.4364       100\n",
            "        house     0.3263    0.6200    0.4276       100\n",
            "     kangaroo     0.2689    0.3200    0.2922       100\n",
            "     keyboard     0.8070    0.4600    0.5860       100\n",
            "         lamp     0.5660    0.3000    0.3922       100\n",
            "   lawn_mower     0.8000    0.6000    0.6857       100\n",
            "      leopard     0.4361    0.5800    0.4979       100\n",
            "         lion     0.3907    0.5900    0.4701       100\n",
            "       lizard     0.3151    0.2300    0.2659       100\n",
            "      lobster     0.3608    0.3500    0.3553       100\n",
            "          man     0.2840    0.4800    0.3569       100\n",
            "   maple_tree     0.3937    0.6300    0.4846       100\n",
            "   motorcycle     0.5337    0.8700    0.6616       100\n",
            "     mountain     0.6262    0.6700    0.6473       100\n",
            "        mouse     0.3704    0.1000    0.1575       100\n",
            "     mushroom     0.2281    0.5200    0.3171       100\n",
            "     oak_tree     0.5440    0.6800    0.6044       100\n",
            "       orange     0.7857    0.5500    0.6471       100\n",
            "       orchid     0.5078    0.6500    0.5702       100\n",
            "        otter     0.1556    0.0700    0.0966       100\n",
            "    palm_tree     0.5763    0.6800    0.6239       100\n",
            "         pear     0.5584    0.4300    0.4859       100\n",
            " pickup_truck     0.4186    0.5400    0.4716       100\n",
            "    pine_tree     0.7200    0.1800    0.2880       100\n",
            "        plain     0.7043    0.8100    0.7535       100\n",
            "        plate     0.4762    0.5000    0.4878       100\n",
            "        poppy     0.4426    0.5400    0.4865       100\n",
            "    porcupine     0.4301    0.4000    0.4145       100\n",
            "       possum     0.2192    0.1600    0.1850       100\n",
            "       rabbit     0.3617    0.1700    0.2313       100\n",
            "      raccoon     0.2074    0.3900    0.2708       100\n",
            "          ray     0.4444    0.3200    0.3721       100\n",
            "         road     0.6807    0.8100    0.7397       100\n",
            "       rocket     0.6238    0.6300    0.6269       100\n",
            "         rose     0.5385    0.4900    0.5131       100\n",
            "          sea     0.5946    0.6600    0.6256       100\n",
            "         seal     0.1935    0.1200    0.1481       100\n",
            "        shark     0.3818    0.2100    0.2710       100\n",
            "        shrew     0.2448    0.3500    0.2881       100\n",
            "        skunk     0.6034    0.7000    0.6481       100\n",
            "   skyscraper     0.6727    0.7400    0.7048       100\n",
            "        snail     0.3651    0.2300    0.2822       100\n",
            "        snake     0.3143    0.2200    0.2588       100\n",
            "       spider     0.5833    0.3500    0.4375       100\n",
            "     squirrel     0.1980    0.2000    0.1990       100\n",
            "    streetcar     0.3488    0.6000    0.4412       100\n",
            "    sunflower     0.6807    0.8100    0.7397       100\n",
            " sweet_pepper     0.3882    0.3300    0.3568       100\n",
            "        table     0.4375    0.4200    0.4286       100\n",
            "         tank     0.6129    0.5700    0.5907       100\n",
            "    telephone     0.4727    0.5200    0.4952       100\n",
            "   television     0.4444    0.7200    0.5496       100\n",
            "        tiger     0.5253    0.5200    0.5226       100\n",
            "      tractor     0.4632    0.6300    0.5339       100\n",
            "        train     0.3600    0.3600    0.3600       100\n",
            "        trout     0.5000    0.5700    0.5327       100\n",
            "        tulip     0.5208    0.2500    0.3378       100\n",
            "       turtle     0.3297    0.3000    0.3141       100\n",
            "     wardrobe     0.7525    0.7600    0.7562       100\n",
            "        whale     0.4750    0.5700    0.5182       100\n",
            "  willow_tree     0.4789    0.3400    0.3977       100\n",
            "         wolf     0.2791    0.3600    0.3144       100\n",
            "        woman     0.2500    0.0800    0.1212       100\n",
            "         worm     0.6667    0.2600    0.3741       100\n",
            "\n",
            "     accuracy                         0.4499     10000\n",
            "    macro avg     0.4614    0.4499    0.4388     10000\n",
            " weighted avg     0.4614    0.4499    0.4388     10000\n",
            "\n",
            "\n",
            "Report saved to /content/drive/MyDrive/ML Assignment 3/cifar100_reports\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train/accuracy</td><td>▁▂▃▄▅▅▅▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/loss</td><td>█▆▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val/accuracy</td><td>▁▂▃▄▄▅▅▆▅▅▆▆▆▇▇▇▇▇██</td></tr><tr><td>val/loss</td><td>█▇▆▅▅▄▄▃▃▃▃▂▃▂▂▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train/accuracy</td><td>35.11111</td></tr><tr><td>train/loss</td><td>2.50362</td></tr><tr><td>val/accuracy</td><td>39.94</td></tr><tr><td>val/loss</td><td>2.33548</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dazzling-wood-14</strong> at: <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/durouo2q' target=\"_blank\">https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/durouo2q</a><br> View project at: <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn' target=\"_blank\">https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn</a><br>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250322_211512-durouo2q/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250322_213523-xrtm2jjn</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/xrtm2jjn' target=\"_blank\">gentle-jazz-15</a></strong> to <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn' target=\"_blank\">https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/xrtm2jjn' target=\"_blank\">https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/xrtm2jjn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with config: {'lr': 0.002, 'batch_size': 32, 'optimizer': 'adam', 'dropout': 0.6, 'epochs': 20}\n",
            "Generating random image samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-0b4a4a8ed1dd>:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "<ipython-input-10-0b4a4a8ed1dd>:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] Batch [0/1407]\n",
            "Epoch [1/20] Batch [100/1407]\n",
            "Epoch [1/20] Batch [200/1407]\n",
            "Epoch [1/20] Batch [300/1407]\n",
            "Epoch [1/20] Batch [400/1407]\n",
            "Epoch [1/20] Batch [500/1407]\n",
            "Epoch [1/20] Batch [600/1407]\n",
            "Epoch [1/20] Batch [700/1407]\n",
            "Epoch [1/20] Batch [800/1407]\n",
            "Epoch [1/20] Batch [900/1407]\n",
            "Epoch [1/20] Batch [1000/1407]\n",
            "Epoch [1/20] Batch [1100/1407]\n",
            "Epoch [1/20] Batch [1200/1407]\n",
            "Epoch [1/20] Batch [1300/1407]\n",
            "Epoch [1/20] Batch [1400/1407]\n",
            "Epoch [1/20]\n",
            "Train Loss: 4.4675, Train Acc: 2.58%\n",
            "Val Loss: 4.2213, Val Acc: 5.76%\n",
            "Epoch [2/20] Batch [0/1407]\n",
            "Epoch [2/20] Batch [100/1407]\n",
            "Epoch [2/20] Batch [200/1407]\n",
            "Epoch [2/20] Batch [300/1407]\n",
            "Epoch [2/20] Batch [400/1407]\n",
            "Epoch [2/20] Batch [500/1407]\n",
            "Epoch [2/20] Batch [600/1407]\n",
            "Epoch [2/20] Batch [700/1407]\n",
            "Epoch [2/20] Batch [800/1407]\n",
            "Epoch [2/20] Batch [900/1407]\n",
            "Epoch [2/20] Batch [1000/1407]\n",
            "Epoch [2/20] Batch [1100/1407]\n",
            "Epoch [2/20] Batch [1200/1407]\n",
            "Epoch [2/20] Batch [1300/1407]\n",
            "Epoch [2/20] Batch [1400/1407]\n",
            "Epoch [2/20]\n",
            "Train Loss: 4.2940, Train Acc: 4.21%\n",
            "Val Loss: 4.0547, Val Acc: 7.50%\n",
            "Epoch [3/20] Batch [0/1407]\n",
            "Epoch [3/20] Batch [100/1407]\n",
            "Epoch [3/20] Batch [200/1407]\n",
            "Epoch [3/20] Batch [300/1407]\n",
            "Epoch [3/20] Batch [400/1407]\n",
            "Epoch [3/20] Batch [500/1407]\n",
            "Epoch [3/20] Batch [600/1407]\n",
            "Epoch [3/20] Batch [700/1407]\n",
            "Epoch [3/20] Batch [800/1407]\n",
            "Epoch [3/20] Batch [900/1407]\n",
            "Epoch [3/20] Batch [1000/1407]\n",
            "Epoch [3/20] Batch [1100/1407]\n",
            "Epoch [3/20] Batch [1200/1407]\n",
            "Epoch [3/20] Batch [1300/1407]\n",
            "Epoch [3/20] Batch [1400/1407]\n",
            "Epoch [3/20]\n",
            "Train Loss: 4.1674, Train Acc: 5.83%\n",
            "Val Loss: 3.9064, Val Acc: 9.88%\n",
            "Epoch [4/20] Batch [0/1407]\n",
            "Epoch [4/20] Batch [100/1407]\n",
            "Epoch [4/20] Batch [200/1407]\n",
            "Epoch [4/20] Batch [300/1407]\n",
            "Epoch [4/20] Batch [400/1407]\n",
            "Epoch [4/20] Batch [500/1407]\n",
            "Epoch [4/20] Batch [600/1407]\n",
            "Epoch [4/20] Batch [700/1407]\n",
            "Epoch [4/20] Batch [800/1407]\n",
            "Epoch [4/20] Batch [900/1407]\n",
            "Epoch [4/20] Batch [1000/1407]\n",
            "Epoch [4/20] Batch [1100/1407]\n",
            "Epoch [4/20] Batch [1200/1407]\n",
            "Epoch [4/20] Batch [1300/1407]\n",
            "Epoch [4/20] Batch [1400/1407]\n",
            "Epoch [4/20]\n",
            "Train Loss: 4.0560, Train Acc: 7.36%\n",
            "Val Loss: 3.7684, Val Acc: 11.44%\n",
            "Epoch [5/20] Batch [0/1407]\n",
            "Epoch [5/20] Batch [100/1407]\n",
            "Epoch [5/20] Batch [200/1407]\n",
            "Epoch [5/20] Batch [300/1407]\n",
            "Epoch [5/20] Batch [400/1407]\n",
            "Epoch [5/20] Batch [500/1407]\n",
            "Epoch [5/20] Batch [600/1407]\n",
            "Epoch [5/20] Batch [700/1407]\n",
            "Epoch [5/20] Batch [800/1407]\n",
            "Epoch [5/20] Batch [900/1407]\n",
            "Epoch [5/20] Batch [1000/1407]\n",
            "Epoch [5/20] Batch [1100/1407]\n",
            "Epoch [5/20] Batch [1200/1407]\n",
            "Epoch [5/20] Batch [1300/1407]\n",
            "Epoch [5/20] Batch [1400/1407]\n",
            "Epoch [5/20]\n",
            "Train Loss: 3.9636, Train Acc: 8.68%\n",
            "Val Loss: 3.6637, Val Acc: 14.00%\n",
            "Epoch [6/20] Batch [0/1407]\n",
            "Epoch [6/20] Batch [100/1407]\n",
            "Epoch [6/20] Batch [200/1407]\n",
            "Epoch [6/20] Batch [300/1407]\n",
            "Epoch [6/20] Batch [400/1407]\n",
            "Epoch [6/20] Batch [500/1407]\n",
            "Epoch [6/20] Batch [600/1407]\n",
            "Epoch [6/20] Batch [700/1407]\n",
            "Epoch [6/20] Batch [800/1407]\n",
            "Epoch [6/20] Batch [900/1407]\n",
            "Epoch [6/20] Batch [1000/1407]\n",
            "Epoch [6/20] Batch [1100/1407]\n",
            "Epoch [6/20] Batch [1200/1407]\n",
            "Epoch [6/20] Batch [1300/1407]\n",
            "Epoch [6/20] Batch [1400/1407]\n",
            "Epoch [6/20]\n",
            "Train Loss: 3.8835, Train Acc: 9.93%\n",
            "Val Loss: 3.5834, Val Acc: 15.72%\n",
            "Epoch [7/20] Batch [0/1407]\n",
            "Epoch [7/20] Batch [100/1407]\n",
            "Epoch [7/20] Batch [200/1407]\n",
            "Epoch [7/20] Batch [300/1407]\n",
            "Epoch [7/20] Batch [400/1407]\n",
            "Epoch [7/20] Batch [500/1407]\n",
            "Epoch [7/20] Batch [600/1407]\n",
            "Epoch [7/20] Batch [700/1407]\n",
            "Epoch [7/20] Batch [800/1407]\n",
            "Epoch [7/20] Batch [900/1407]\n",
            "Epoch [7/20] Batch [1000/1407]\n",
            "Epoch [7/20] Batch [1100/1407]\n",
            "Epoch [7/20] Batch [1200/1407]\n",
            "Epoch [7/20] Batch [1300/1407]\n",
            "Epoch [7/20] Batch [1400/1407]\n",
            "Epoch [7/20]\n",
            "Train Loss: 3.8195, Train Acc: 10.82%\n",
            "Val Loss: 3.5262, Val Acc: 16.42%\n",
            "Epoch [8/20] Batch [0/1407]\n",
            "Epoch [8/20] Batch [100/1407]\n",
            "Epoch [8/20] Batch [200/1407]\n",
            "Epoch [8/20] Batch [300/1407]\n",
            "Epoch [8/20] Batch [400/1407]\n",
            "Epoch [8/20] Batch [500/1407]\n",
            "Epoch [8/20] Batch [600/1407]\n",
            "Epoch [8/20] Batch [700/1407]\n",
            "Epoch [8/20] Batch [800/1407]\n",
            "Epoch [8/20] Batch [900/1407]\n",
            "Epoch [8/20] Batch [1000/1407]\n",
            "Epoch [8/20] Batch [1100/1407]\n",
            "Epoch [8/20] Batch [1200/1407]\n",
            "Epoch [8/20] Batch [1300/1407]\n",
            "Epoch [8/20] Batch [1400/1407]\n",
            "Epoch [8/20]\n",
            "Train Loss: 3.7760, Train Acc: 11.54%\n",
            "Val Loss: 3.4695, Val Acc: 18.66%\n",
            "Epoch [9/20] Batch [0/1407]\n",
            "Epoch [9/20] Batch [100/1407]\n",
            "Epoch [9/20] Batch [200/1407]\n",
            "Epoch [9/20] Batch [300/1407]\n",
            "Epoch [9/20] Batch [400/1407]\n",
            "Epoch [9/20] Batch [500/1407]\n",
            "Epoch [9/20] Batch [600/1407]\n",
            "Epoch [9/20] Batch [700/1407]\n",
            "Epoch [9/20] Batch [800/1407]\n",
            "Epoch [9/20] Batch [900/1407]\n",
            "Epoch [9/20] Batch [1000/1407]\n",
            "Epoch [9/20] Batch [1100/1407]\n",
            "Epoch [9/20] Batch [1200/1407]\n",
            "Epoch [9/20] Batch [1300/1407]\n",
            "Epoch [9/20] Batch [1400/1407]\n",
            "Epoch [9/20]\n",
            "Train Loss: 3.7253, Train Acc: 12.33%\n",
            "Val Loss: 3.3886, Val Acc: 19.26%\n",
            "Epoch [10/20] Batch [0/1407]\n",
            "Epoch [10/20] Batch [100/1407]\n",
            "Epoch [10/20] Batch [200/1407]\n",
            "Epoch [10/20] Batch [300/1407]\n",
            "Epoch [10/20] Batch [400/1407]\n",
            "Epoch [10/20] Batch [500/1407]\n",
            "Epoch [10/20] Batch [600/1407]\n",
            "Epoch [10/20] Batch [700/1407]\n",
            "Epoch [10/20] Batch [800/1407]\n",
            "Epoch [10/20] Batch [900/1407]\n",
            "Epoch [10/20] Batch [1000/1407]\n",
            "Epoch [10/20] Batch [1100/1407]\n",
            "Epoch [10/20] Batch [1200/1407]\n",
            "Epoch [10/20] Batch [1300/1407]\n",
            "Epoch [10/20] Batch [1400/1407]\n",
            "Epoch [10/20]\n",
            "Train Loss: 3.6744, Train Acc: 13.38%\n",
            "Val Loss: 3.3552, Val Acc: 20.48%\n",
            "Epoch [11/20] Batch [0/1407]\n",
            "Epoch [11/20] Batch [100/1407]\n",
            "Epoch [11/20] Batch [200/1407]\n",
            "Epoch [11/20] Batch [300/1407]\n",
            "Epoch [11/20] Batch [400/1407]\n",
            "Epoch [11/20] Batch [500/1407]\n",
            "Epoch [11/20] Batch [600/1407]\n",
            "Epoch [11/20] Batch [700/1407]\n",
            "Epoch [11/20] Batch [800/1407]\n",
            "Epoch [11/20] Batch [900/1407]\n",
            "Epoch [11/20] Batch [1000/1407]\n",
            "Epoch [11/20] Batch [1100/1407]\n",
            "Epoch [11/20] Batch [1200/1407]\n",
            "Epoch [11/20] Batch [1300/1407]\n",
            "Epoch [11/20] Batch [1400/1407]\n",
            "Epoch [11/20]\n",
            "Train Loss: 3.6371, Train Acc: 13.91%\n",
            "Val Loss: 3.3015, Val Acc: 21.40%\n",
            "Epoch [12/20] Batch [0/1407]\n",
            "Epoch [12/20] Batch [100/1407]\n",
            "Epoch [12/20] Batch [200/1407]\n",
            "Epoch [12/20] Batch [300/1407]\n",
            "Epoch [12/20] Batch [400/1407]\n",
            "Epoch [12/20] Batch [500/1407]\n",
            "Epoch [12/20] Batch [600/1407]\n",
            "Epoch [12/20] Batch [700/1407]\n",
            "Epoch [12/20] Batch [800/1407]\n",
            "Epoch [12/20] Batch [900/1407]\n",
            "Epoch [12/20] Batch [1000/1407]\n",
            "Epoch [12/20] Batch [1100/1407]\n",
            "Epoch [12/20] Batch [1200/1407]\n",
            "Epoch [12/20] Batch [1300/1407]\n",
            "Epoch [12/20] Batch [1400/1407]\n",
            "Epoch [12/20]\n",
            "Train Loss: 3.5932, Train Acc: 14.58%\n",
            "Val Loss: 3.2930, Val Acc: 21.26%\n",
            "Epoch [13/20] Batch [0/1407]\n",
            "Epoch [13/20] Batch [100/1407]\n",
            "Epoch [13/20] Batch [200/1407]\n",
            "Epoch [13/20] Batch [300/1407]\n",
            "Epoch [13/20] Batch [400/1407]\n",
            "Epoch [13/20] Batch [500/1407]\n",
            "Epoch [13/20] Batch [600/1407]\n",
            "Epoch [13/20] Batch [700/1407]\n",
            "Epoch [13/20] Batch [800/1407]\n",
            "Epoch [13/20] Batch [900/1407]\n",
            "Epoch [13/20] Batch [1000/1407]\n",
            "Epoch [13/20] Batch [1100/1407]\n",
            "Epoch [13/20] Batch [1200/1407]\n",
            "Epoch [13/20] Batch [1300/1407]\n",
            "Epoch [13/20] Batch [1400/1407]\n",
            "Epoch [13/20]\n",
            "Train Loss: 3.5634, Train Acc: 14.91%\n",
            "Val Loss: 3.2447, Val Acc: 21.56%\n",
            "Epoch [14/20] Batch [0/1407]\n",
            "Epoch [14/20] Batch [100/1407]\n",
            "Epoch [14/20] Batch [200/1407]\n",
            "Epoch [14/20] Batch [300/1407]\n",
            "Epoch [14/20] Batch [400/1407]\n",
            "Epoch [14/20] Batch [500/1407]\n",
            "Epoch [14/20] Batch [600/1407]\n",
            "Epoch [14/20] Batch [700/1407]\n",
            "Epoch [14/20] Batch [800/1407]\n",
            "Epoch [14/20] Batch [900/1407]\n",
            "Epoch [14/20] Batch [1000/1407]\n",
            "Epoch [14/20] Batch [1100/1407]\n",
            "Epoch [14/20] Batch [1200/1407]\n",
            "Epoch [14/20] Batch [1300/1407]\n",
            "Epoch [14/20] Batch [1400/1407]\n",
            "Epoch [14/20]\n",
            "Train Loss: 3.5129, Train Acc: 15.74%\n",
            "Val Loss: 3.2183, Val Acc: 22.64%\n",
            "Epoch [15/20] Batch [0/1407]\n",
            "Epoch [15/20] Batch [100/1407]\n",
            "Epoch [15/20] Batch [200/1407]\n",
            "Epoch [15/20] Batch [300/1407]\n",
            "Epoch [15/20] Batch [400/1407]\n",
            "Epoch [15/20] Batch [500/1407]\n",
            "Epoch [15/20] Batch [600/1407]\n",
            "Epoch [15/20] Batch [700/1407]\n",
            "Epoch [15/20] Batch [800/1407]\n",
            "Epoch [15/20] Batch [900/1407]\n",
            "Epoch [15/20] Batch [1000/1407]\n",
            "Epoch [15/20] Batch [1100/1407]\n",
            "Epoch [15/20] Batch [1200/1407]\n",
            "Epoch [15/20] Batch [1300/1407]\n",
            "Epoch [15/20] Batch [1400/1407]\n",
            "Epoch [15/20]\n",
            "Train Loss: 3.5002, Train Acc: 16.25%\n",
            "Val Loss: 3.1799, Val Acc: 23.38%\n",
            "Epoch [16/20] Batch [0/1407]\n",
            "Epoch [16/20] Batch [100/1407]\n",
            "Epoch [16/20] Batch [200/1407]\n",
            "Epoch [16/20] Batch [300/1407]\n",
            "Epoch [16/20] Batch [400/1407]\n",
            "Epoch [16/20] Batch [500/1407]\n",
            "Epoch [16/20] Batch [600/1407]\n",
            "Epoch [16/20] Batch [700/1407]\n",
            "Epoch [16/20] Batch [800/1407]\n",
            "Epoch [16/20] Batch [900/1407]\n",
            "Epoch [16/20] Batch [1000/1407]\n",
            "Epoch [16/20] Batch [1100/1407]\n",
            "Epoch [16/20] Batch [1200/1407]\n",
            "Epoch [16/20] Batch [1300/1407]\n",
            "Epoch [16/20] Batch [1400/1407]\n",
            "Epoch [16/20]\n",
            "Train Loss: 3.4596, Train Acc: 16.86%\n",
            "Val Loss: 3.1339, Val Acc: 23.62%\n",
            "Epoch [17/20] Batch [0/1407]\n",
            "Epoch [17/20] Batch [100/1407]\n",
            "Epoch [17/20] Batch [200/1407]\n",
            "Epoch [17/20] Batch [300/1407]\n",
            "Epoch [17/20] Batch [400/1407]\n",
            "Epoch [17/20] Batch [500/1407]\n",
            "Epoch [17/20] Batch [600/1407]\n",
            "Epoch [17/20] Batch [700/1407]\n",
            "Epoch [17/20] Batch [800/1407]\n",
            "Epoch [17/20] Batch [900/1407]\n",
            "Epoch [17/20] Batch [1000/1407]\n",
            "Epoch [17/20] Batch [1100/1407]\n",
            "Epoch [17/20] Batch [1200/1407]\n",
            "Epoch [17/20] Batch [1300/1407]\n",
            "Epoch [17/20] Batch [1400/1407]\n",
            "Epoch [17/20]\n",
            "Train Loss: 3.4414, Train Acc: 17.42%\n",
            "Val Loss: 3.1104, Val Acc: 24.62%\n",
            "Epoch [18/20] Batch [0/1407]\n",
            "Epoch [18/20] Batch [100/1407]\n",
            "Epoch [18/20] Batch [200/1407]\n",
            "Epoch [18/20] Batch [300/1407]\n",
            "Epoch [18/20] Batch [400/1407]\n",
            "Epoch [18/20] Batch [500/1407]\n",
            "Epoch [18/20] Batch [600/1407]\n",
            "Epoch [18/20] Batch [700/1407]\n",
            "Epoch [18/20] Batch [800/1407]\n",
            "Epoch [18/20] Batch [900/1407]\n",
            "Epoch [18/20] Batch [1000/1407]\n",
            "Epoch [18/20] Batch [1100/1407]\n",
            "Epoch [18/20] Batch [1200/1407]\n",
            "Epoch [18/20] Batch [1300/1407]\n",
            "Epoch [18/20] Batch [1400/1407]\n",
            "Epoch [18/20]\n",
            "Train Loss: 3.4152, Train Acc: 17.61%\n",
            "Val Loss: 3.0231, Val Acc: 26.00%\n",
            "Epoch [19/20] Batch [0/1407]\n",
            "Epoch [19/20] Batch [100/1407]\n",
            "Epoch [19/20] Batch [200/1407]\n",
            "Epoch [19/20] Batch [300/1407]\n",
            "Epoch [19/20] Batch [400/1407]\n",
            "Epoch [19/20] Batch [500/1407]\n",
            "Epoch [19/20] Batch [600/1407]\n",
            "Epoch [19/20] Batch [700/1407]\n",
            "Epoch [19/20] Batch [800/1407]\n",
            "Epoch [19/20] Batch [900/1407]\n",
            "Epoch [19/20] Batch [1000/1407]\n",
            "Epoch [19/20] Batch [1100/1407]\n",
            "Epoch [19/20] Batch [1200/1407]\n",
            "Epoch [19/20] Batch [1300/1407]\n",
            "Epoch [19/20] Batch [1400/1407]\n",
            "Epoch [19/20]\n",
            "Train Loss: 3.3919, Train Acc: 17.80%\n",
            "Val Loss: 3.0095, Val Acc: 24.70%\n",
            "Epoch [20/20] Batch [0/1407]\n",
            "Epoch [20/20] Batch [100/1407]\n",
            "Epoch [20/20] Batch [200/1407]\n",
            "Epoch [20/20] Batch [300/1407]\n",
            "Epoch [20/20] Batch [400/1407]\n",
            "Epoch [20/20] Batch [500/1407]\n",
            "Epoch [20/20] Batch [600/1407]\n",
            "Epoch [20/20] Batch [700/1407]\n",
            "Epoch [20/20] Batch [800/1407]\n",
            "Epoch [20/20] Batch [900/1407]\n",
            "Epoch [20/20] Batch [1000/1407]\n",
            "Epoch [20/20] Batch [1100/1407]\n",
            "Epoch [20/20] Batch [1200/1407]\n",
            "Epoch [20/20] Batch [1300/1407]\n",
            "Epoch [20/20] Batch [1400/1407]\n",
            "Epoch [20/20]\n",
            "Train Loss: 3.3681, Train Acc: 18.48%\n",
            "Val Loss: 3.0025, Val Acc: 26.06%\n",
            "\n",
            "Generating classification report...\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        apple     0.4257    0.6300    0.5081       100\n",
            "aquarium_fish     0.3736    0.3400    0.3560       100\n",
            "         baby     0.2121    0.2800    0.2414       100\n",
            "         bear     0.1739    0.0800    0.1096       100\n",
            "       beaver     0.0909    0.0200    0.0328       100\n",
            "          bed     0.1688    0.1300    0.1469       100\n",
            "          bee     0.4571    0.1600    0.2370       100\n",
            "       beetle     0.3167    0.1900    0.2375       100\n",
            "      bicycle     0.4247    0.3100    0.3584       100\n",
            "       bottle     0.3478    0.3200    0.3333       100\n",
            "         bowl     0.2000    0.0200    0.0364       100\n",
            "          boy     0.4000    0.0600    0.1043       100\n",
            "       bridge     0.4194    0.2600    0.3210       100\n",
            "          bus     0.1786    0.2000    0.1887       100\n",
            "    butterfly     0.1280    0.2100    0.1591       100\n",
            "        camel     0.3065    0.1900    0.2346       100\n",
            "          can     0.3151    0.2300    0.2659       100\n",
            "       castle     0.5000    0.6900    0.5798       100\n",
            "  caterpillar     0.3243    0.1200    0.1752       100\n",
            "       cattle     0.3429    0.2400    0.2824       100\n",
            "        chair     0.5203    0.6400    0.5740       100\n",
            "   chimpanzee     0.2624    0.5800    0.3614       100\n",
            "        clock     0.2262    0.1900    0.2065       100\n",
            "        cloud     0.4661    0.5500    0.5046       100\n",
            "    cockroach     0.2472    0.6700    0.3612       100\n",
            "        couch     0.3333    0.0300    0.0550       100\n",
            "         crab     0.1951    0.0800    0.1135       100\n",
            "    crocodile     0.1132    0.2400    0.1538       100\n",
            "          cup     0.3148    0.1700    0.2208       100\n",
            "     dinosaur     0.2632    0.1500    0.1911       100\n",
            "      dolphin     0.1968    0.4900    0.2808       100\n",
            "     elephant     0.2439    0.3000    0.2691       100\n",
            "     flatfish     0.2826    0.1300    0.1781       100\n",
            "       forest     0.4222    0.1900    0.2621       100\n",
            "          fox     0.2788    0.2900    0.2843       100\n",
            "         girl     0.1682    0.5400    0.2565       100\n",
            "      hamster     0.1917    0.3700    0.2526       100\n",
            "        house     0.3684    0.1400    0.2029       100\n",
            "     kangaroo     0.1565    0.1800    0.1674       100\n",
            "     keyboard     0.6071    0.1700    0.2656       100\n",
            "         lamp     0.6667    0.0200    0.0388       100\n",
            "   lawn_mower     0.4359    0.5100    0.4700       100\n",
            "      leopard     0.2358    0.5000    0.3205       100\n",
            "         lion     0.2487    0.4700    0.3253       100\n",
            "       lizard     0.1707    0.0700    0.0993       100\n",
            "      lobster     0.3077    0.0800    0.1270       100\n",
            "          man     0.0513    0.0200    0.0288       100\n",
            "   maple_tree     0.7045    0.3100    0.4306       100\n",
            "   motorcycle     0.3671    0.7600    0.4951       100\n",
            "     mountain     0.5714    0.3200    0.4103       100\n",
            "        mouse     0.0000    0.0000    0.0000       100\n",
            "     mushroom     0.1899    0.1500    0.1676       100\n",
            "     oak_tree     0.4863    0.7100    0.5772       100\n",
            "       orange     0.3750    0.7800    0.5065       100\n",
            "       orchid     0.3909    0.4300    0.4095       100\n",
            "        otter     0.0727    0.0400    0.0516       100\n",
            "    palm_tree     0.5821    0.3900    0.4671       100\n",
            "         pear     0.3412    0.2900    0.3135       100\n",
            " pickup_truck     0.2512    0.5100    0.3366       100\n",
            "    pine_tree     0.2857    0.1600    0.2051       100\n",
            "        plain     0.7292    0.7000    0.7143       100\n",
            "        plate     0.3483    0.3100    0.3280       100\n",
            "        poppy     0.5000    0.2300    0.3151       100\n",
            "    porcupine     0.2370    0.3200    0.2723       100\n",
            "       possum     0.0795    0.1900    0.1121       100\n",
            "       rabbit     0.1250    0.0200    0.0345       100\n",
            "      raccoon     0.1524    0.2500    0.1894       100\n",
            "          ray     0.2414    0.3500    0.2857       100\n",
            "         road     0.5430    0.8200    0.6534       100\n",
            "       rocket     0.4242    0.5600    0.4828       100\n",
            "         rose     0.4000    0.2000    0.2667       100\n",
            "          sea     0.5439    0.6200    0.5794       100\n",
            "         seal     0.1111    0.0100    0.0183       100\n",
            "        shark     0.2903    0.1800    0.2222       100\n",
            "        shrew     0.1613    0.2000    0.1786       100\n",
            "        skunk     0.2620    0.7100    0.3827       100\n",
            "   skyscraper     0.6667    0.5200    0.5843       100\n",
            "        snail     0.2000    0.0500    0.0800       100\n",
            "        snake     0.1579    0.0600    0.0870       100\n",
            "       spider     0.2632    0.2000    0.2273       100\n",
            "     squirrel     0.1296    0.0700    0.0909       100\n",
            "    streetcar     0.2791    0.1200    0.1678       100\n",
            "    sunflower     0.4901    0.7400    0.5896       100\n",
            " sweet_pepper     0.3061    0.1500    0.2013       100\n",
            "        table     0.4000    0.1200    0.1846       100\n",
            "         tank     0.3302    0.3500    0.3398       100\n",
            "    telephone     0.3065    0.3800    0.3393       100\n",
            "   television     0.3517    0.5100    0.4163       100\n",
            "        tiger     0.1775    0.5200    0.2646       100\n",
            "      tractor     0.3272    0.5300    0.4046       100\n",
            "        train     0.2237    0.1700    0.1932       100\n",
            "        trout     0.2783    0.3200    0.2977       100\n",
            "        tulip     0.2466    0.1800    0.2081       100\n",
            "       turtle     0.2000    0.0400    0.0667       100\n",
            "     wardrobe     0.4863    0.7100    0.5772       100\n",
            "        whale     0.3106    0.4100    0.3534       100\n",
            "  willow_tree     0.3684    0.2800    0.3182       100\n",
            "         wolf     0.1486    0.2600    0.1891       100\n",
            "        woman     0.1395    0.0600    0.0839       100\n",
            "         worm     0.4000    0.0200    0.0381       100\n",
            "\n",
            "     accuracy                         0.2974     10000\n",
            "    macro avg     0.3104    0.2974    0.2719     10000\n",
            " weighted avg     0.3104    0.2974    0.2719     10000\n",
            "\n",
            "\n",
            "Report saved to /content/drive/MyDrive/ML Assignment 3/cifar100_reports\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train/accuracy</td><td>▁▂▂▃▄▄▅▅▅▆▆▆▆▇▇▇████</td></tr><tr><td>train/loss</td><td>█▇▆▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val/accuracy</td><td>▁▂▂▃▄▄▅▅▆▆▆▆▆▇▇▇████</td></tr><tr><td>val/loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train/accuracy</td><td>18.48</td></tr><tr><td>train/loss</td><td>3.36807</td></tr><tr><td>val/accuracy</td><td>26.06</td></tr><tr><td>val/loss</td><td>3.00249</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gentle-jazz-15</strong> at: <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/xrtm2jjn' target=\"_blank\">https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/xrtm2jjn</a><br> View project at: <a href='https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn' target=\"_blank\">https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn</a><br>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250322_213523-xrtm2jjn/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://wandb.ai/abuzarkhan-indraprastha-institute-of-information-technol/cifar100-cnn/runs/4q9gkawj?nw=nwuserabuzarkhan"
      ],
      "metadata": {
        "id": "C0SlA5bLeReQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}